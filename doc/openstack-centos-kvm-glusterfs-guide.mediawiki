<p></p>
<h1 id="introduction">Introduction</h1>
<p>The Cloud computing model leverages virtualization to deliver computing resources to users on-demand on a pay-per-use basis <span class="citation">[1], [2]</span>. It provides the properties of self-service and elasticity enabling the users to dynamically and flexibly adjust their resource consumption according to the current workload. These properties of the Cloud computing model allow one to avoid high upfront investments in a computing infrastructure, thus reducing the time to market and facilitating a higher pace of innovation.</p>
<p>Cloud computing resources are delivered to the users through three major service models:</p>
<ul>
<li><em>Infrastructure as a Service (IaaS)</em>: computing resources are delivered in the form of Virtual Machines (VMs). A VM provides to the user a view of a dedicated hardware. The user is capable of managing the system within a VM and deploying the required software. Examples of IaaS are Amazon EC2<sup><a href="#fn1" class="footnoteRef" id="fnref1">1</a></sup> and Google Compute Engine<sup><a href="#fn2" class="footnoteRef" id="fnref2">2</a></sup>.</li>
<li><em>Platform as a Service (PaaS)</em>: the access to the resources is provided in the form of an Application Programming Interface (API) that is used for application development and deployment. In this model, the user does not have a direct access to the system resources, rather the resource allocation to applications is automatically managed by the platform. Examples of PaaS are Google App Engine<sup><a href="#fn3" class="footnoteRef" id="fnref3">3</a></sup> and Microsoft Azure<sup><a href="#fn4" class="footnoteRef" id="fnref4">4</a></sup>.</li>
<li><em>Software as a Service (SaaS)</em>: application-level software services are provided to the users on a subscription bases over the Internet. Examples of SaaS are Salesforce.com<sup><a href="#fn5" class="footnoteRef" id="fnref5">5</a></sup> and applications from the Amazon Web Services Marketplace<sup><a href="#fn6" class="footnoteRef" id="fnref6">6</a></sup>.</li>
</ul>
<p>In this work, we focus on the low level service model -- IaaS. Apart from the service models, Cloud computing services are distinguished according to their deployment models. There are three basic deployment models:</p>
<ul>
<li><em>Public Cloud</em>: computing resources are provided publicly over the Internet based on a pay-per-use model.</li>
<li><em>Private Cloud</em>: the Cloud infrastructure is owned by an organization, and hosted and operated internally.</li>
<li><em>Hybrid Cloud</em>: computing resources are provided by a composition of a private and public Clouds.</li>
</ul>
<p>Public Clouds, such as Amazon EC2, have initiated and driven the industrial adoption of the Cloud computing model. However, the software platforms utilized by public Cloud providers are usually proprietary disallowing their deployment on-premise. In other words, due to closed-source software, it is not possible to deploy the same software platform used, for example, by Amazon EC2 on a private computing infrastructure. Fortunately, there exist several open source Cloud platforms striving to address the issue, such as OpenStack, CloudStack, Eucalyptus, and OpenNebula. The mentioned projects basically allow anyone to not only deploy a private Cloud environment free of charge, but also contribute back to the development of the platform.</p>
<p>The aim of this work is to facilitate further development and adoption of open source Cloud computing software by providing a step-by-step guide to installing OpenStack on multiple computing nodes using a set of shell scripts. The difference from the existing tools for automated installation of OpenStack is that the purpose of this work is not only obtaining a fully operational OpenStack Cloud environment, but also learning the steps required to perform the installation from the ground up and understanding the responsibilities and interaction of the OpenStack components. This is achieved by splitting the installation process into multiple logical steps, and implementing each step as a separate shell script. In this paper, we go through and discuss each of the complete sequence of steps required to install OpenStack on top of CentOS 6.3 using the Kernel-based Virtual Machine (KVM) as a hypervisor and GlusterFS as a distributed replicated file system to enable live migration and provide fault tolerance. The source code described in this paper is released under the Apache 2.0 License and is publicly available online<sup><a href="#fn7" class="footnoteRef" id="fnref7">7</a></sup>.</p>
<p>In summary, this paper discusses and guides through the installation process of the following software:</p>
<ul>
<li>CentOS<sup><a href="#fn8" class="footnoteRef" id="fnref8">8</a></sup>: a free Linux Operating System (OS) distribution derived from the Red Hat Enterprise Linux (RHEL) distribution.</li>
<li>GlusterFS<sup><a href="#fn9" class="footnoteRef" id="fnref9">9</a></sup>: a distributed file system providing shared replicated storage across multiple servers over Ethernet or Infiniband. Having a storage shared between the compute nodes is a requirement for enabling live migration of VM instances. However, having a centralized shared storage service, such as NAS limits the scalability and leads to a single point of failure. In contrast, the advantages of a distributed file system solution, such as GlusterFS, are: (1) no single point of failure, which means even if a server fails, the storage and data will remain available due to automatic replication over multiple servers; (2) higher scalability, as Input/Output (I/O) operations are distributed across multiple servers; and (3) due to the data replication over multiple servers, if a data replica is available on the host, VM instances access the data locally rather than remotely over network improving the I/O performance.</li>
<li>KVM<sup><a href="#fn10" class="footnoteRef" id="fnref10">10</a></sup>: a hypervisor providing full virtualization for Linux leveraging hardware-assisted virtualization support of the Intel VT and AMD-V chipsets. The kernel component of KVM is included in the Linux kernel since the 2.6.20 version.</li>
<li>OpenStack<sup><a href="#fn11" class="footnoteRef" id="fnref11">11</a></sup>: free open source IaaS Cloud computing software originally released by Rackspace and NASA under the Apache 2.0 License in July 2010. The OpenStack project is currently lead and managed by the OpenStack Foundation, which is &quot;an independent body providing shared resources to help achieve the OpenStack Mission by Protecting, Empowering, and Promoting OpenStack software and the community around it, including users, developers and the entire ecosystem&quot;.<sup><a href="#fn12" class="footnoteRef" id="fnref12">12</a></sup></li>
</ul>
<p>In the next section we give an overview of the OpenStack software, its features, main components, and their interaction. In Section 3, we briefly compare 4 open source Cloud computing platforms, namely OpenStack, Eucalyptus, CloudStack, and OpenNebula. In Section 4, we discuss the existing tools for automated installation of OpenStack and the differences from our approach. In Section 5 we provide a detailed description and discussion of the steps required to install OpenStack on top of CentOS using KVM and GlusterFS. In Section 6, we conclude the paper with a summary and discussion of future directions.</p>
<h1 id="overview-of-the-openstack-cloud-platform">Overview of the OpenStack Cloud Platform</h1>
<div class="figure">
<img src="openstack-software-diagram.png" alt="A high level view of the OpenStack service interaction [3]" /><p class="caption">A high level view of the OpenStack service interaction <span class="citation">[3]</span></p>
</div>
<p>OpenStack is a free open source IaaS Cloud platform originally released by Rackspace and NASA under the Apache 2.0 License in July 2010. OpenStack controls and manages compute, storage, and network resource aggregated from multiple servers in a data center. The system provides a web interface (dashboard) and APIs compatible with Amazon EC2 to the administrators and users that allow flexible on-demand provisioning of the resources.</p>
<p>In April 2012, the project lead and management functions have been transferred to a newly formed OpenStack Foundation. The goals of the foundation are to support an open development process and community building, drive awareness and adoption, and encourage and maintain an ecosystem of companies powered by the OpenStack software. The OpenStack project is currently supported by more than 150 companies including AMD, Intel, Canonical, SUSE Linux, Red Hat, Cisco, Dell, HP, IBM and Yahoo!.</p>
<p>The OpenStack software is divided into several services shown in Figure 1 that through their interaction provide the overall system management capabilities. The main services include the following:</p>
<ul>
<li><em>OpenStack Compute (Nova)</em>: manages the life cycle of VM instances from scheduling and resource provisioning to live migration and security rules. By leveraging the virtualization API provided by Libvirt<sup><a href="#fn13" class="footnoteRef" id="fnref13">13</a></sup>, OpenStack Compute supports multiple hypervisors, such as KVM and Xen.</li>
<li><em>OpenStack Storage</em>: provides block and object storage to use by VM instances. The block storage system allows the uses to create block storage devices and dynamically attach and detach them from VM instances using the dashboard or API. In addition to block storage, OpenStack provides a scalable distributed object storage called Swift, which is also accessible through an API.</li>
<li><em>OpenStack Networking</em>: provides API-driven network and IP address management capabilities. The system allows the users to create their own networks and assign static, floating, or dynamic IP addresses to VM instances.</li>
<li><em>OpenStack Dashboard (Horizon)</em>: provides a web interface for the administrators and users to the system management capabilities, such as VM image management, VM instance life cycle management, and storage management.</li>
<li><em>OpenStack Identity (Keystone)</em>: a centralized user account management service acting as an authentication and access control system. In addition, the service provides the access to a registry of the OpenStack services deployed in the data center and their communication endpoints.</li>
<li><em>OpenStack Image (Glance)</em>: provides various VM image management capabilities, such as registration, delivery, and snapshotting. The service supports multiple VM image formats including Raw, AMI, VHD, VDI, qcow2, VMDK, and OVF.</li>
</ul>
<p>The OpenStack software is architectured with an aim of providing decoupling between the services constituting the system. The services interact with each other through the public APIs they provide and using Keystone as a registry for obtaining the information about the communication endpoints. The OpenStack Compute service, also referred to as Nova, is built on a shared-nothing messaging-based architecture, which allows running the services on multiple servers. The services, which compose Nova communicate via the Advanced Message Queue Protocol (AMQP) using asynchronous calls to avoid blocking. More detailed information on installation and administration of OpenStack in given in the official manuals <span class="citation">[4], [5]</span>. In the next section we compare OpenStack with the other major open source Cloud platforms.</p>
<h1 id="comparison-of-open-source-cloud-platforms">Comparison of Open Source Cloud Platforms</h1>
<ul>
<li>OpenStack</li>
<li>Eucalyptus</li>
<li>CloudStack</li>
<li>OpenNebula</li>
</ul>
<h1 id="existing-openstack-installation-tools">Existing OpenStack Installation Tools</h1>
<ul>
<li>DevStack<sup><a href="#fn14" class="footnoteRef" id="fnref14">14</a></sup></li>
<li>Puppet / Chef<sup><a href="#fn15" class="footnoteRef" id="fnref15">15</a></sup></li>
<li>Difference From our Approach</li>
<li>The purpose is not just having an up and running OpenStack installation, but also learning the steps required to perform the installation from the ground up and understanding the responsibilities and interaction of the OpenStack components.</li>
</ul>
<h1 id="step-by-step-openstack-installation">Step-by-Step OpenStack Installation</h1>
<p>As mentioned earlier, the aim of this work is to detail the steps required to perform a complete installation of OpenStack on multiple nodes. We split the installation process into multiple subsequent logical steps and provide a shell script for each of the steps. In this section, we explain and discuss every step needed to be followed to obtain a fully operational OpenStack installation on our testbed consisting of 1 controller and 4 compute nodes. The source code of the shell scripts described in this paper is released under the Apache 2.0 License and is publicly available online<sup><a href="#fn16" class="footnoteRef" id="fnref16">16</a></sup>.</p>
<h2 id="hardware-setup">Hardware Setup</h2>
<p>The testbed used for testing the installation scripts consists of the following hardware:</p>
<ul>
<li>1 x Dell Optiplex 745
<ul>
<li>Intel(R) Core(TM) 2 CPU (2 cores, 2 threads) 6600 @ 2.40GHz</li>
<li>2GB DDR2-667</li>
<li>Seagate Barracuda 80GB, 7200 RPM SATA II (ST3808110AS)</li>
<li>Broadcom 5751 NetXtreme Gigabit Controller</li>
</ul></li>
<li>4 x IBM System x3200 M3
<ul>
<li>Intel(R) Xeon(R) CPU (4 cores, 8 threads), X3460 @ 2.80GHz</li>
<li>4GB DDR3-1333</li>
<li>Western Digital 250 GB, 7200 RPM SATA II (WD2502ABYS-23B7A)</li>
<li>Dual Gigabit Ethernet (2 x Intel 82574L Ethernet Controller)</li>
</ul></li>
<li>1 x Netgear ProSafe 16-Port 10/100 Desktop Switch FS116</li>
</ul>
<p>The Dell Optiplex 745 machine has been chosen to serve as a management host running all the major OpenStack services. The management host is referred to as the <em>controller</em> further in the text. The 4 IBM System x3200 M3 servers are used as <em>compute hosts</em>, i.e. for hosting VM instances.</p>
<p>Due to the specifics of our setup, the only one machine connected to the public network and the Internet is one of the IBM System x3200 M3 servers. This server is refereed to as the <em>gateway</em>. The gateway is connected to the public network via the eth0 network interface.</p>
<p>All the machines form a local network connected via the Netgear FS116 network switch. The compute hosts are connected to the local network through their eth1 network interfaces. The controller is connected to the local network through its eth0 interface. To provide the access to the public network and the Internet, the gateway performs Network Address Translation (NAT) for the hosts from the local network.</p>
<h2 id="organization-of-the-installation-package">Organization of the Installation Package</h2>
<p>The project contains a number of directories, whose organization is explained in this section. The <code>config</code> directory includes configuration files, which are used by the installation scripts and should be modified prior to the installation. The <code>lib</code> directory contains utility scripts that are shared by the other installation scripts. The <code>doc</code> directory comprises the source and compiled versions of the documentation.</p>
<p>The remaining directories directly include the step-by-step installation scripts. The names of these directories have a specific format. The prefix (before the first dash) is the number denoting the order of execution. For example, the scripts from the directory with the prefix <em>01</em> must be executed first, followed by the scripts from the directory with the prefix <em>02</em>, etc. The middle part of a directory name denotes the purpose of the scripts in this directory. The suffix (after the last dash) specifies the host, on which the scripts from this directory should be executed. There are 4 possible values of the target host prefix:</p>
<ul>
<li><em>all</em> -- execute the scripts on all the hosts;</li>
<li><em>compute</em> -- execute the scripts on all the compute hosts;</li>
<li><em>controller</em> -- execute the scripts on the controller;</li>
<li><em>gateway</em> -- execute the scripts on the gateway.</li>
</ul>
<p>For example, the first directory is named <code>01-network-gateway</code>, which means that (1) the scripts from this directory must be executed in the first place; (2) the scripts are supposed to do a network set up; and (3) the scripts must be executed only on the gateway. The name <code>02-glusterfs-all</code> means: (1) the scripts from this directory must be executed after the scripts from <code>01-network-gateway</code>; (2) the scripts set up GlusterFS; and (3) the scripts must be executed on all the hosts.</p>
<p>The names of the installation scripts themselves follow a similar convention. The prefix denotes the order, in which the scripts should be run, while the remaining part of the name describes the purpose of the script.</p>
<h2 id="configuration-files">Configuration Files</h2>
<p>The <code>lib</code> directory contains configuration files used by the installation scripts. These configuration files should be modified prior to running the installation scripts. The configuration files are described below.</p>
<dl>
<dt><code>configrc:</code></dt>
<dd><p>This file contains a number of environmental variables defining various aspects of OpenStack's configuration, such as administration and service account credentials, as well as access points. The file must be &quot;sourced&quot; to export the variables into the current shell session. The file can be sourced directly by running: <code>. configrc</code>, or using the scripts described later. A simple test to check whether the variables have been correctly exported is to <code>echo</code> any of the variables. For example, <code>echo $OS_USERNAME</code> must output <code>admin</code> for the default configuration.</p>
</dd>
<dt><code>hosts:</code></dt>
<dd><p>This files contains a mapping between the IP addresses of the hosts in the local network and their host names. We apply the following host name convention: the compute hosts are named <em>computeX</em>, where <em>X</em> is replaced by the number of the host. According the described hardware setup, the default configuration defines 4 compute hosts: <code>compute1</code> (192.168.0.1), <code>compute2</code> (192.168.0.2), <code>compute3</code> (192.168.0.3), <code>compute4</code> (192.168.0.4); and 1 <code>controller</code> (192.168.0.5). As mentioned above, in our setup one of the compute hosts is connected to the public network and acts as a gateway. We assign to this host the host name <code>compute1</code>, and also alias it as <code>gateway</code>.</p>
</dd>
<dt><code>ntp.conf:</code></dt>
<dd><p>This files contains a list of Network Time Protocol (NTP) servers to use by all the hosts. It is important to set accessible servers, since time synchronization is important for OpenStack services to interact correctly. By default, this file defines servers used within the University of Melbourne. It is advised to replace the default configuration with a list of preferred servers.</p>
</dd>
</dl>
<p>It is important to replaced the default configuration defined in the described configuration files, since the default configuration is tailored to the specific setup of our testbed.</p>
<h2 id="installation-procedure">Installation Procedure</h2>
<h3 id="centos">CentOS</h3>
<p>The installation scripts have been tested with CentOS 6.3, which has been installed on all the hosts. The CentOS installation mainly follows the standard process described in detail in the Red Hat Enterprise Linux 6 Installation Guide <span class="citation">[6]</span>. The steps of the installation process that differ from the standard are discussed in this section.</p>
<h4 id="network-configuration.">Network Configuration.</h4>
<p>The simplest way to configure network is during the OS installation process. As mentioned above, in our setup, the gateway is connected to two networks: to the public network through the eth0 interface; and to the local network through the eth1 interface. Since in our setup the public network configuration can be obtain from a DHCP server, in the configuration of the eth0 interface it is only required to enable automatic connection by enabling the &quot;Connect Automatically&quot; option. We use static configuration for the local network; therefore, eth1 has be configured manually. Apart from enabling the &quot;Connect Automatically&quot; option, it is necessary to configure IPv4 by adding an IP address and netmask. According to the configuration defined in the <code>hosts</code> file described above, we assign 192.168.0.1/24 to the gateway.</p>
<p>One of the differences in the network configuration of the other compute hosts (<code>compute2</code>, <code>compute3</code>, and <code>compute4</code>) from the gateway is that eth0 should be kept disabled, as it is unused. The eth1 interface should be enabled by turning on the &quot;Connect Automatically&quot; option. The IP address and netmask for eth1 should be set to 192.168.0.<em>X</em>/24, where <em>X</em> is replaced by the compute host number. The gateway for the compute hosts should be set to 192.168.0.1, which the IP address of the gateway. The controller is configured similarly to the compute hosts with the only difference that the configuration should be done for eth0 instead of eth1, since the controller has only one network interface.</p>
<h4 id="hard-drive-partitioning.">Hard Drive Partitioning.</h4>
<p>The hard drive partitioning scheme is the same for all the compute hosts, but differs for the controller. Table 1 shows the partitioning scheme for the compute hosts. <code>vg_base</code> is a volume group comprising the standard OS partitions: <code>lv_root</code>, <code>lv_home</code> and <code>lv_swap</code>. <code>vg_gluster</code> is a special volume group containing a single <code>lv_gluster</code> partition, which is dedicated to serve as a GlusterFS brick. The <code>lv_gluster</code> logical volume is formatted using the XFS<sup><a href="#fn17" class="footnoteRef" id="fnref17">17</a></sup> file system, as recommended for GlusterFS bricks.</p>
<table>
<caption>The partitioning scheme for the compute hosts</caption>
<col width="30%" />
<col width="15%" />
<col width="29%" />
<col width="13%" />
<thead>
<tr class="header">
<th align="left">Device</th>
<th align="left">Size (MB)</th>
<th align="left">Mount Point / Volume</th>
<th align="left">Type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><em>LVM Volume Groups</em></td>
<td align="left">
</td>
<td align="left">
</td>
<td align="left">
</td>
</tr>
<tr class="even">
<td align="left">  vg_base</td>
<td align="left">20996</td>
<td align="left">
</td>
<td align="left">
</td>
</tr>
<tr class="odd">
<td align="left">    lv_root</td>
<td align="left">10000</td>
<td align="left">/</td>
<td align="left">ext4</td>
</tr>
<tr class="even">
<td align="left">    lv_swap</td>
<td align="left">6000</td>
<td align="left">
</td>
<td align="left">swap</td>
</tr>
<tr class="odd">
<td align="left">    lv_home</td>
<td align="left">4996</td>
<td align="left">/home</td>
<td align="left">ext4</td>
</tr>
<tr class="even">
<td align="left">  vg_gluster</td>
<td align="left">216972</td>
<td align="left">
</td>
<td align="left">
</td>
</tr>
<tr class="odd">
<td align="left">    lv_gluster</td>
<td align="left">216972</td>
<td align="left">/export/gluster</td>
<td align="left">xfs</td>
</tr>
<tr class="even">
<td align="left"><em>Hard Drives</em></td>
<td align="left">
</td>
<td align="left">
</td>
<td align="left">
</td>
</tr>
<tr class="odd">
<td align="left">  sda</td>
<td align="left">
</td>
<td align="left">
</td>
<td align="left">
</td>
</tr>
<tr class="even">
<td align="left">    sda1</td>
<td align="left">500</td>
<td align="left">/boot</td>
<td align="left">ext4</td>
</tr>
<tr class="odd">
<td align="left">    sda2</td>
<td align="left">21000</td>
<td align="left">vg_base</td>
<td align="left">PV (LVM)</td>
</tr>
<tr class="even">
<td align="left">    sda3</td>
<td align="left">216974</td>
<td align="left">vg_gluster</td>
<td align="left">PV (LVM)</td>
</tr>
</tbody>
</table>
<p>Table 2 shows the partitioning scheme for the controller. It does not include a <code>vg_gluster</code> volume group since the controller is not going to be a part of the GlusterFS volume. However, the scheme includes two new important volume groups: <code>nova-volumes</code> and <code>vg_images</code>. The <code>nova-volumes</code> volume group is used by OpenStack Nova to allocated volumes for VM instances. This volume group is managed by Nova; therefore, there is not need to create logical volumes manually. The <code>vg_images</code> volume group and its <code>lv_images</code> logical volume are devoted for storing VM images by OpenStack Glance. The mount point for <code>lv_images</code> is <code>/var/lib/glance/images</code>, which is the default directory used by Glance to store VM image files.</p>
<table>
<caption>The partitioning scheme for the controller</caption>
<col width="27%" />
<col width="15%" />
<col width="31%" />
<col width="13%" />
<thead>
<tr class="header">
<th align="left">Device</th>
<th align="left">Size (MB)</th>
<th align="left">Mount Point / Volume</th>
<th align="left">Type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><em>LVM Volume Groups</em></td>
<td align="left">
</td>
<td align="left">
</td>
<td align="left">
</td>
</tr>
<tr class="even">
<td align="left">  nova-volumes</td>
<td align="left">29996</td>
<td align="left">
</td>
<td align="left">
</td>
</tr>
<tr class="odd">
<td align="left">    Free</td>
<td align="left">29996</td>
<td align="left">
</td>
<td align="left">
</td>
</tr>
<tr class="even">
<td align="left">  vg_base</td>
<td align="left">16996</td>
<td align="left">
</td>
<td align="left">
</td>
</tr>
<tr class="odd">
<td align="left">    lv_root</td>
<td align="left">10000</td>
<td align="left">/</td>
<td align="left">ext4</td>
</tr>
<tr class="even">
<td align="left">    lv_swap</td>
<td align="left">2000</td>
<td align="left">
</td>
<td align="left">swap</td>
</tr>
<tr class="odd">
<td align="left">    lv_home</td>
<td align="left">4996</td>
<td align="left">/home</td>
<td align="left">ext4</td>
</tr>
<tr class="even">
<td align="left">  vg_images</td>
<td align="left">28788</td>
<td align="left">
</td>
<td align="left">
</td>
</tr>
<tr class="odd">
<td align="left">    lv_images</td>
<td align="left">28788</td>
<td align="left">/var/lib/glance/images</td>
<td align="left">ext4</td>
</tr>
<tr class="even">
<td align="left"><em>Hard Drives</em></td>
<td align="left">
</td>
<td align="left">
</td>
<td align="left">
</td>
</tr>
<tr class="odd">
<td align="left">  sda</td>
<td align="left">
</td>
<td align="left">
</td>
<td align="left">
</td>
</tr>
<tr class="even">
<td align="left">    sda1</td>
<td align="left">500</td>
<td align="left">/boot</td>
<td align="left">ext4</td>
</tr>
<tr class="odd">
<td align="left">    sda2</td>
<td align="left">17000</td>
<td align="left">vg_base</td>
<td align="left">PV (LVM)</td>
</tr>
<tr class="even">
<td align="left">    sda3</td>
<td align="left">30000</td>
<td align="left">nova-volumes</td>
<td align="left">PV (LVM)</td>
</tr>
<tr class="odd">
<td align="left">    sda4</td>
<td align="left">28792</td>
<td align="left">
</td>
<td align="left">Extended</td>
</tr>
<tr class="even">
<td align="left">      sda5</td>
<td align="left">28788</td>
<td align="left">vg_images</td>
<td align="left">PV (LVM)</td>
</tr>
</tbody>
</table>
<h3 id="network-gateway">Network Gateway</h3>
<p>Once CentOS is installed on all the machines, the next step is to configure NAT on the gateway to enable the Internet access on all the hosts. First, it is necessary to check whether the Internet is available on the gateway itself. If the Internet is not available, the problem might be in the configuration of eth0, the network interface connected to the public network in our setup.</p>
<p>In all the following steps, it is assumed that the user logged in is <code>root</code>. If the Internet is available on the gateway, it is necessary to install the Git<sup><a href="#fn18" class="footnoteRef" id="fnref18">18</a></sup> version control client to be able to clone the repository containing the installation scripts. This can be done using <code>yum</code>, the default package manager in CentOS, as follows:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash">yum <span class="kw">install</span> -y git</code></pre>
<p>Next, the repository can be cloned using the following command:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash">git clone <span class="kw">\</span>
   https://github.com/beloglazov/openstack-centos-kvm-glusterfs.git</code></pre>
<p>Now, we can continue the installation using the scripts contained in the cloned Git repository. As described above, the starting point is the directory called <code>01-network-gateway</code>.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="kw">cd</span> openstack-centos-kvm-glusterfs/01-network-gateway</code></pre>
<p>All the scripts described below can be run by executing <code>./&lt;script name&gt;.sh</code> in the command line.</p>
<ol style="list-style-type: example">
<li><code>01-iptables-nat.sh</code></li>
</ol>
<p>This script flushes all the default <code>iptables</code> rules to open all the ports. This is acceptable for testing; however, it is not recommended for production environments due to security concerns. Then, the script sets up NAT using <code>iptables</code> by forwarding packets from eth1 (the local network interface) through eth0. The last stage is saving the defined <code>iptables</code> rules and restarting the service.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Flush the iptables rules.</span>
iptables -F
iptables -t nat -F
iptables -t mangle -F

<span class="co"># Set up packet forwarding for NAT</span>
iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE
iptables -A FORWARD -i eth1 -j ACCEPT
iptables -A FORWARD -o eth1 -j ACCEPT

<span class="co"># Save the iptables configuration into a file and restart iptables</span>
service iptables save
service iptables restart</code></pre>
<ol start="2" style="list-style-type: example">
<li><code>02-ip-forward.sh</code></li>
</ol>
<p>By default, IP packet forwarding is disabled in CentOS; therefore, it is necessary to enable it by modifying the <code>/etc/sysctl.conf</code> configuration file. This is done by the <code>02-ip-forward.sh</code> script as follows:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Enable IP packet forwarding</span>
<span class="kw">sed</span> -i <span class="st">&#39;s/net.ipv4.ip_forward = 0/net.ipv4.ip_forward = 1/g&#39;</span> <span class="kw">\</span>
   /etc/sysctl.conf

<span class="co"># Restart the network service</span>
service network restart</code></pre>
<ol start="3" style="list-style-type: example">
<li><code>03-copy-hosts.sh</code></li>
</ol>
<p>This script copies the <code>hosts</code> file from the <code>config</code> directory to <code>/etc</code> locally, as well to all the other hosts: the remaining compute hosts and the controller. The <code>hosts</code> file defines a mapping between the IP addresses of the hosts and host names. For convenience, prior to copying you may use the <code>ssh-copy-id</code> program to copy the public key to the other hosts for password-less SSH connections. Once the <code>hosts</code> file is copied to all the hosts, they can be accessed by using their respective host names instead of the IP addresses.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Copy the hosts file into the local configuration</span>
<span class="kw">cp</span> ../config/hosts /etc/

<span class="co"># Copy the hosts file to the other nodes.</span>
<span class="kw">scp</span> ../config/hosts root@compute2:/etc/
<span class="kw">scp</span> ../config/hosts root@compute3:/etc/
<span class="kw">scp</span> ../config/hosts root@compute4:/etc/
<span class="kw">scp</span> ../config/hosts root@controller:/etc/</code></pre>
<p>From this point, all the installation steps on any host can be performed remotely over SSH.</p>
<h3 id="glusterfs-distributed-replicated-storage">GlusterFS Distributed Replicated Storage</h3>
<p>In this section, we describe how to set up distributed replicated storage using GlusterFS.</p>
<h4 id="glusterfs-all-all-nodes.">02-glusterfs-all (all nodes).</h4>
<p>The steps discussed in this section need to be run on all the hosts. The easiest way to manage multi-node installation is to SSH into all the hosts from another machine using separate terminals. This way the hosts can be conveniently managed from a single machine simultaneously. Before applying further installation scripts, it is necessary to run the following commands:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Update the OS packages</span>
yum update -y

<span class="co"># Install Git</span>
yum <span class="kw">install</span> -y git

<span class="co"># Clone the repository</span>
git clone <span class="kw">\</span>
   https://github.com/beloglazov/openstack-centos-kvm-glusterfs.git</code></pre>
<p>It is optional but might be useful to install other programs on all the hosts, such as <code>man</code>, <code>nano</code>, or <code>emacs</code> for reading manuals and editing files.</p>
<ol start="4" style="list-style-type: example">
<li><code>01-iptables-flush.sh</code></li>
</ol>
<p>This script flushes all the default <code>iptables</code> rules to allow connections through all the ports. As mentioned above, this is insecure and not recommended for production environments. For production it is recommended to open only the required ports.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Flush the iptables rules.</span>
iptables -F

<span class="co"># Save the configuration and restart iptables</span>
service iptables save
service iptables restart</code></pre>
<ol start="5" style="list-style-type: example">
<li><code>02-selinux-permissive.sh</code></li>
</ol>
<p>This script switches SELinux<sup><a href="#fn19" class="footnoteRef" id="fnref19">19</a></sup> into the permissive mode. By default, SELinux blocks certain operations, such as VM migrations. Switching SELinux into the permissive mode is not recommended for production environments, but is acceptable for testing purposes.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Set SELinux into the permissive mode</span>
<span class="kw">sed</span> -i <span class="st">&#39;s/SELINUX=enforcing/SELINUX=permissive/g&#39;</span> /etc/selinux/config
<span class="kw">echo</span> 0 <span class="kw">&gt;</span> /selinux/enforce</code></pre>
<ol start="6" style="list-style-type: example">
<li><code>03-glusterfs-install.sh</code></li>
</ol>
<p>This script installs GlusterFS services and their dependencies.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Install GlusterFS and its dependencies</span>
yum -y <span class="kw">install</span> <span class="kw">\</span>
   openssh-server <span class="kw">wget</span> fuse fuse-libs openib libibverbs <span class="kw">\</span>
   http://download.gluster.org/pub/gluster/glusterfs/LATEST/<span class="kw">\</span>
      CentOS/glusterfs-3.3.0-1.el6.x86_64.rpm <span class="kw">\</span>
   http://download.gluster.org/pub/gluster/glusterfs/LATEST/<span class="kw">\</span>
      CentOS/glusterfs-fuse-3.3.0-1.el6.x86_64.rpm <span class="kw">\</span>
   http://download.gluster.org/pub/gluster/glusterfs/LATEST/<span class="kw">\</span>
      CentOS/glusterfs-server-3.3.0-1.el6.x86_64.rpm</code></pre>
<ol start="7" style="list-style-type: example">
<li><code>04-glusterfs-start.sh</code></li>
</ol>
<p>This script starts the GlusterFS service, and sets the service to start during the system start up.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Start the GlusterFS service</span>
service glusterd restart
chkconfig glusterd on</code></pre>
<h4 id="glusterfs-controller-controller.">03-glusterfs-controller (controller).</h4>
<p>The scripts described in this section need to be run only on the controller.</p>
<ol start="8" style="list-style-type: example">
<li><code>01-glusterfs-probe.sh</code></li>
</ol>
<p>This script probes the compute hosts to add them to a GlusterFS cluster.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Probe GlusterFS peer hosts</span>
gluster peer probe compute1
gluster peer probe compute2
gluster peer probe compute3
gluster peer probe compute4</code></pre>
<ol start="9" style="list-style-type: example">
<li><code>02-glusterfs-create-volume.sh</code></li>
</ol>
<p>This scripts creates a GlusterFS volume out of the bricks exported by the compute hosts mounted to <code>/export/gluster</code> for storing VM instances. The created GlusterFS volume is replicated across all the 4 compute hosts. Such replication provides fault tolerance, as if any of the compute hosts fail, the VM instance data will be available from the remaining replicas. Compared to a Network File System (NFS) exported by a single server, the complete replication provided by GlusterFS improves the read performance, since all the read operations are local. This is important to enable efficient live migration of VMs.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Create a GlusterFS volume replicated over 4 gluster hosts</span>
gluster volume create vm-instances replica 4 <span class="kw">\</span>
   compute1:/export/gluster compute2:/export/gluster <span class="kw">\</span>
   compute3:/export/gluster compute4:/export/gluster

<span class="co"># Start the created volume</span>
gluster volume start vm-instances</code></pre>
<h4 id="glusterfs-all-all-nodes.-1">04-glusterfs-all (all nodes).</h4>
<p>The script described in this section needs to be run on all the hosts.</p>
<ol start="10" style="list-style-type: example">
<li><code>01-glusterfs-mount.sh</code></li>
</ol>
<p>This scripts adds a line to the <code>/etc/fstab</code> configuration file to automatically mount the GlusterFS volume during the system start up to the <code>/var/lib/nova/instances</code> directory. The <code>/var/lib/nova/instances</code> directory is the default location where OpenStack Nova stores the VM instance related data. This directory must be mounted and shared by the controller and all the compute hosts to enable live migration of VMs. Even though the controller does not manage the data of VM instances, it is still necessary for it to have the access to the VM instance data directory to enable live migration. The reason is that the controller coordinates live migration by writing some temporary data to the shared directory. The <code>mount -a</code> command re-mounts everything from the config file after it has been modified.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Mount the GlusterFS volume</span>
<span class="kw">mkdir</span> -p /var/lib/nova/instances
<span class="kw">echo</span> <span class="st">&quot;localhost:/vm-instances /var/lib/nova/instances \</span>
<span class="st">   glusterfs defaults 0 0&quot;</span> <span class="kw">&gt;&gt;</span> /etc/fstab
<span class="kw">mount</span> -a</code></pre>
<h3 id="kvm">KVM</h3>
<p>The scripts included in the <code>05-kvm-compute</code> directory need to be run on the compute hosts. KVM is not required on the controller, since it is not going to be used to host VM instances.</p>
<p>Prior to enabling KVM on a machine, it is important to make sure that the machine uses either Intel VT or AMD-V chipsets that support hardware-assisted virtualization. This feature might be disabled in the Basic Input Output System (BIOS); therefore, it is necessary to verify that it is enabled. To check whether hardware-assisted virtualization is supported by the hardware, the following Linux command can be used:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="kw">grep</span> -E <span class="st">&#39;vmx|svm&#39;</span> /proc/cpuinfo</code></pre>
<p>If the command returns any output, it means that the system supports hardware-assisted virtualization. The <code>vmx</code> processor feature flag represents an Intel VT chipset, whereas the <code>svm</code> flag represents AMD-V. Depending on the flag returned, you need to modify the <code>02-kvm-modprobe.sh</code> script.</p>
<ol start="11" style="list-style-type: example">
<li><code>01-kvm-install.sh</code></li>
</ol>
<p>This script installs KVM and the related tools.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Install KVM and the related tools</span>
yum -y <span class="kw">install</span> kvm qemu-kvm qemu-kvm-tools</code></pre>
<ol start="12" style="list-style-type: example">
<li><code>02-kvm-modprobe.sh</code></li>
</ol>
<p>This script enables KVM in the OS. If the <code>grep -E 'vmx|svm' /proc/cpuinfo</code> command described above returned <code>vmx</code>, there is no need to modify this script, as it enables the Intel KVM module by default. If the command returned <code>svm</code>, it is necessary to comment the <code>modprobe kvm-intel</code> line and uncomment the <code>modprobe kvm-amd</code> line.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Create a script for enabling the KVM kernel module</span>
<span class="kw">echo</span> <span class="st">&quot;</span>
<span class="st">modprobe kvm</span>

<span class="st"># Uncomment this line if the host has an AMD CPU</span>
<span class="st">#modprobe kvm-amd</span>

<span class="st"># Uncomment this line if the host has an Intel CPU</span>
<span class="st">modprobe kvm-intel</span>
<span class="st">&quot;</span> <span class="kw">&gt;</span> /etc/sysconfig/modules/kvm.modules

<span class="kw">chmod</span> +x /etc/sysconfig/modules/kvm.modules

<span class="co"># Enable KVM</span>
/etc/sysconfig/modules/kvm.modules</code></pre>
<ol start="13" style="list-style-type: example">
<li><code>03-libvirt-install.sh</code></li>
</ol>
<p>This script installs Libvirt<sup><a href="#fn20" class="footnoteRef" id="fnref20">20</a></sup>, its dependencies and the related tools. Libvirt provides an abstraction and a common Application Programming Interface (API) over various hypervisors. It is used by OpenStack to provide support for multiple hypervisors including KVM and Xen. After the installation, the script starts the <code>messagebus</code> and <code>avahi-daemon</code> services, which are prerequisites of Libvirt.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Install Libvirt and its dependencies</span>
yum -y <span class="kw">install</span> libvirt libvirt-python python-virtinst avahi dmidecode

<span class="co"># Start the services required by Libvirt</span>
service messagebus restart
service avahi-daemon restart

<span class="co"># Start the service during the system start up</span>
chkconfig messagebus on
chkconfig avahi-daemon on</code></pre>
<ol start="14" style="list-style-type: example">
<li><code>04-libvirt-config.sh</code></li>
</ol>
<p>This script modifies the Libvirt configuration to enable communication over TCP without authentication. This is required by OpenStack to enable live migration of VM instances.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Enable the communication with Libvirt</span>
<span class="co"># over TCP without authentication.</span>
<span class="kw">sed</span> -i <span class="st">&#39;s/#listen_tls = 0/listen_tls = 0/g&#39;</span> <span class="kw">\</span>
   /etc/libvirt/libvirtd.conf
<span class="kw">sed</span> -i <span class="st">&#39;s/#listen_tcp = 1/listen_tcp = 1/g&#39;</span> <span class="kw">\</span>
   /etc/libvirt/libvirtd.conf
<span class="kw">sed</span> -i <span class="st">&#39;s/#auth_tcp = &quot;sasl&quot;/auth_tcp = &quot;none&quot;/g&#39;</span> <span class="kw">\</span>
   /etc/libvirt/libvirtd.conf
<span class="kw">sed</span> -i <span class="st">&#39;s/#LIBVIRTD_ARGS=&quot;--listen&quot;/LIBVIRTD_ARGS=&quot;--listen&quot;/g&#39;</span> <span class="kw">\</span>
   /etc/sysconfig/libvirtd</code></pre>
<ol start="15" style="list-style-type: example">
<li><code>05-libvirt-start.sh</code></li>
</ol>
<p>This script starts the <code>libvirtd</code> service and sets it to automatically start during the system start up.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Start the Libvirt service</span>
service libvirtd restart
chkconfig libvirtd on</code></pre>
<h3 id="openstack">OpenStack</h3>
<p>This section contains a few subsection describing different phases of OpenStack installation.</p>
<h4 id="openstack-all-all-nodes.">06-openstack-all (all nodes).</h4>
<p>The scripts described in this section need to be executed on all the hosts.</p>
<ol start="16" style="list-style-type: example">
<li><code>01-epel-add-repo.sh</code></li>
</ol>
<p>This scripts adds the Extra Packages for Enterprise Linux<sup><a href="#fn21" class="footnoteRef" id="fnref21">21</a></sup> (EPEL) repository, which contains the OpenStack related packages.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Add the EPEL repo: http://fedoraproject.org/wiki/EPEL</span>
yum <span class="kw">install</span> -y http://dl.fedoraproject.org/pub/epel/6/i386/<span class="kw">\</span>
   epel-release-6-7.noarch.rpm</code></pre>
<ol start="17" style="list-style-type: example">
<li><code>02-ntp-install.sh</code></li>
</ol>
<p>This script install the NTP service, which is required to automatically synchronize the time with external NTP servers.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Install NTP</span>
yum <span class="kw">install</span> -y ntp</code></pre>
<ol start="18" style="list-style-type: example">
<li><code>03-ntp-config.sh</code></li>
</ol>
<p>This script replaces the default servers specified in the <code>/etc/ntp.conf</code> configuration file with the servers specified in the <code>config/ntp.conf</code> file described above. If the default set of servers is satisfactory, then the execution of this script is not required.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Fetch the NTP servers specified in ../config/ntp.conf</span>
<span class="ot">SERVER1=</span><span class="kw">`cat</span> ../config/ntp.conf <span class="kw">|</span> <span class="kw">sed</span> <span class="st">&#39;1!d;q&#39;</span><span class="kw">`</span>
<span class="ot">SERVER2=</span><span class="kw">`cat</span> ../config/ntp.conf <span class="kw">|</span> <span class="kw">sed</span> <span class="st">&#39;2!d;q&#39;</span><span class="kw">`</span>
<span class="ot">SERVER3=</span><span class="kw">`cat</span> ../config/ntp.conf <span class="kw">|</span> <span class="kw">sed</span> <span class="st">&#39;3!d;q&#39;</span><span class="kw">`</span>

<span class="co"># Replace the default NTP servers with the above</span>
<span class="kw">sed</span> -i <span class="st">&quot;s/server 0.*pool.ntp.org/</span><span class="ot">$SERVER1</span><span class="st">/g&quot;</span> /etc/ntp.conf
<span class="kw">sed</span> -i <span class="st">&quot;s/server 1.*pool.ntp.org/</span><span class="ot">$SERVER2</span><span class="st">/g&quot;</span> /etc/ntp.conf
<span class="kw">sed</span> -i <span class="st">&quot;s/server 2.*pool.ntp.org/</span><span class="ot">$SERVER3</span><span class="st">/g&quot;</span> /etc/ntp.conf</code></pre>
<ol start="19" style="list-style-type: example">
<li><code>04-ntp-start.sh</code></li>
</ol>
<p>This script starts the <code>ntpdate</code> service and sets it to start during the system start up. Upon the start, the <code>ntpdate</code> service synchronizes the time with the servers specified in the <code>/etc/ntp.conf</code> configuration file.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Start the NTP service</span>
service ntpdate restart
chkconfig ntpdate on</code></pre>
<h4 id="openstack-controller-controller.">07-openstack-controller (controller).</h4>
<p>The scripts described in this section need to be run only on the controller host.</p>
<ol start="20" style="list-style-type: example">
<li><code>01-source-configrc.sh</code></li>
</ol>
<p>This scripts is mainly used to remind of the necessity to &quot;source&quot; the <code>configrc</code> file prior to continuing, since some scripts in this directory use the environmental variable defined in <code>configrc</code>. To source the file, it is necessary to run the following command: <code>. 01-source-configrc.sh</code>.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="kw">echo</span> <span class="st">&quot;To make the environmental variables available \</span>
<span class="st">   in the current session, run: &quot;</span>
<span class="kw">echo</span> <span class="st">&quot;. 01-source-configrc.sh&quot;</span>

<span class="co"># Export the variables defined in ../config/configrc</span>
<span class="kw">.</span> ../config/configrc</code></pre>
<ol start="21" style="list-style-type: example">
<li><code>02-mysql-install.sh</code></li>
</ol>
<p>This script installs the MySQL server, which is required to host the databases used by the OpenStack services.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Install the MySQL server</span>
yum <span class="kw">install</span> -y mysql mysql-server</code></pre>
<ol start="22" style="list-style-type: example">
<li><code>03-mysql-start.sh</code></li>
</ol>
<p>This script start the MySQL service and initializes the password of the <code>root</code> MySQL user using a variable from the <code>configrc</code> file called <code>$MYSQL_ROOT_PASSWORD</code>.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Start the MySQL service</span>
service mysqld start
chkconfig mysqld on

<span class="co"># Initialize the MySQL root password</span>
mysqladmin -u root password <span class="ot">$MYSQL_ROOT_PASSWORD</span>

<span class="kw">echo</span> <span class="st">&quot;&quot;</span>
<span class="kw">echo</span> <span class="st">&quot;The MySQL root password has been set \</span>
<span class="st">   to the value of </span><span class="ot">$MYSQL_ROOT_PASSWORD</span><span class="st">: </span><span class="dt">\&quot;</span><span class="ot">$MYSQL_ROOT_PASSWORD</span><span class="dt">\&quot;</span><span class="st">&quot;</span></code></pre>
<ol start="23" style="list-style-type: example">
<li><code>04-keystone-install.sh</code></li>
</ol>
<p>This script installs Keystone - the OpenStack identity management service, and other OpenStack command line utilities.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Install OpenStack utils and Keystone, the identity management service</span>
yum <span class="kw">install</span> -y openstack-utils openstack-keystone</code></pre>
<ol start="24" style="list-style-type: example">
<li><code>05-keystone-create-db.sh</code></li>
</ol>
<p>This script creates a MySQL database for Keystone called <code>keystone</code>, which is used to store various identity data. The script also creates a <code>keystone</code> user and grants the user with full permissions to the <code>keystone</code> database.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Create a database for Keystone</span>
../lib/mysqlq.sh <span class="st">&quot;CREATE DATABASE keystone;&quot;</span>

<span class="co"># Create a keystone user and grant all privileges</span>
<span class="co"># to the keystone database</span>
../lib/mysqlq.sh <span class="st">&quot;GRANT ALL ON keystone.* TO &#39;keystone&#39;@&#39;controller&#39; \</span>
<span class="st">   IDENTIFIED BY &#39;</span><span class="ot">$KEYSTONE_MYSQL_PASSWORD</span><span class="st">&#39;;&quot;</span></code></pre>
<ol start="25" style="list-style-type: example">
<li><code>06-keystone-generate-admin-token.sh</code></li>
</ol>
<p>Keystone allows two types of authentication for administrative action like creating users, tenants, etc:</p>
<ol style="list-style-type: decimal">
<li><p>Using an admin token and <code>admin_port</code> (35357), e.g.:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash">keystone <span class="kw">\</span>
   --token=<span class="kw">&lt;</span>admin token<span class="kw">&gt;</span> <span class="kw">\</span>
   --endpoint=http://controller:35357/v2.0 user-list</code></pre></li>
<li><p>Using an admin user and <code>public_port</code> (5000), e.g.:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash">keystone <span class="kw">\</span>
   --os_username=admin <span class="kw">\</span>
   --os_tenant_name=admin <span class="kw">\</span>
   --os_password=<span class="kw">&lt;</span>password<span class="kw">&gt;</span> <span class="kw">\</span>
   --os_auth_url=http://controller:5000/v2.0 user-list</code></pre></li>
</ol>
<p>Services, such as Glance and Nova, can also authenticate in Keystone using one of two ways. One way is to share the admin token among the services and authenticate using the token. However, it is also possible to use special users created in Keystone for each service. By default, these users are nova, glance, etc. The service users are assigned to the service tenant and admin role in that tenant.</p>
<p>Here is an example of the password-based authenication for nova:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash">    nova <span class="kw">\</span>
       --os_username=nova <span class="kw">\</span>
       --os_password=<span class="kw">&lt;</span>password<span class="kw">&gt;</span> <span class="kw">\</span>
       --os_tenant_name=service <span class="kw">\</span>
       --os_auth_url=http://controller:5000/v2.0 list</code></pre>
<p>One of two sets of authentication parameters is required to be specified in <code>/etc/nova/api-paste.ini</code>. The first option is to set up the token-based authentication, like the following:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash">auth_host = controller
auth_protocol = http
admin_token = <span class="kw">&lt;</span>admin token<span class="kw">&gt;</span></code></pre>
<p>The second option is to set up the password-based authentication, as follows:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash">auth_host = controller
auth_protocol = http
admin_tenant_name = service
admin_user = nova
admin_password = <span class="kw">&lt;</span>password<span class="kw">&gt;</span></code></pre>
<p>The password-based authentication might be preferable, since it uses Keystone's database backend to store user credentials. Therefore, it is possible to update user credentials, for example, using Keystone's command line tools without the necessity to re-generate the admin token and update the configuration files.</p>
<p>Even though, the user name and password are specified in the config file, it is still necessary to provide these data when using the command line tools. One way to do this is to directly provide the credentials in the form of command line arguments, as shown above. Another approach, which we apply in this work, is to set corresponding environmental variables that will be automatically used by the command line tools.</p>
<p>The <code>06-keystone-generate-admin-token.sh</code> script generates a random token used to authorize the Keystone admin account. The generated token is stored in the <code>./keystone-admin-token</code> file.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Generate an admin token for Keystone and save it into</span>
<span class="co"># ./keystone-admin-token</span>
openssl rand -hex 10 <span class="kw">&gt;</span> keystone-admin-token</code></pre>
<ol start="26" style="list-style-type: example">
<li><code>07-keystone-config.sh</code></li>
</ol>
<p>This script modifies the configuration file of Keystone, <code>/etc/keystone/keystone.conf</code>. It sets the generated admin token and the MySQL connection configuration using the variables defined in <code>configrc</code>.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Set the generated admin token in the Keystone configuration</span>
openstack-config --set /etc/keystone/keystone.conf DEFAULT <span class="kw">\</span>
   admin_token <span class="kw">`cat</span> keystone-admin-token<span class="kw">`</span>

<span class="co"># Set the connection to the MySQL server</span>
openstack-config --set /etc/keystone/keystone.conf sql connection <span class="kw">\</span>
   mysql://keystone:<span class="ot">$KEYSTONE_MYSQL_PASSWORD</span>@controller/keystone</code></pre>
<ol start="27" style="list-style-type: example">
<li><code>08-keystone-init-db.sh</code></li>
</ol>
<p>This script initializes the <code>keystone</code> database using the <code>keystone-manage</code> command line tool. The executed command creates tables in the database.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Initialize the database for Keystone</span>
keystone-manage db_sync</code></pre>
<ol start="28" style="list-style-type: example">
<li><code>09-keystone-permissions.sh</code></li>
</ol>
<p>This script sets restrictive permissions (640) on the Keystone configuration file, since it contains the MySQL account credentials and the admin token. Then, the scripts sets the ownership of the Keystone related directories to the <code>keystone</code> user and <code>keystone</code> group.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Set restrictive permissions on the Keystone config file</span>
<span class="kw">chmod</span> 640 /etc/keystone/keystone.conf

<span class="co"># Set the ownership for the Keystone related directories</span>
<span class="kw">chown</span> -R keystone:keystone /var/log/keystone
<span class="kw">chown</span> -R keystone:keystone /var/lib/keystone</code></pre>
<ol start="29" style="list-style-type: example">
<li><code>10-keystone-start.sh</code></li>
</ol>
<p>This script starts the Keystone service and sets it to automatically start during the system start up.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Start the Keystone service</span>
service openstack-keystone restart
chkconfig openstack-keystone on</code></pre>
<ol start="30" style="list-style-type: example">
<li><code>11-keystone-create-users.sh</code></li>
</ol>
<p>The purpose of this script is to create user accounts, roles and tenants in Keystone for the admin user and service accounts for each OpenStack service: Keystone, Glance, and Nova. Since the process is complicated when done manually (it is necessary to define relations between database records), we use the <em>keystone-init</em> project<sup><a href="#fn22" class="footnoteRef" id="fnref22">22</a></sup> to automate the process. The <em>keystone-init</em> project allows one to create a configuration file in the &quot;YAML Ain't Markup Language&quot;<sup><a href="#fn23" class="footnoteRef" id="fnref23">23</a></sup> (YAML) data format defining the required OpenStack user accounts. Then, according the defined configuration, the required database records are automatically created.</p>
<p>Our script first installs a dependency of <em>keystone-init</em> and clones the project's repository. Then, the script modifies the default configuration file provided with the <em>keystone-init</em> project by populating it with the values defined by the environmental variables defined in <code>configrc</code>. The last step of the script is to invoke <em>keystone-init</em>. The script does not remove the <em>keystone-init</em> repository to allow one to browse the generated configuration file, e.g. to check the correctness. When the repository is not required anymore, it can be removed by executing <code>rm -rf keystone-init</code>.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Install PyYAML, a YAML Python library</span>
yum <span class="kw">install</span> -y PyYAML

<span class="co"># Clone a repository with Keystone initialization scripts</span>
git clone https://github.com/nimbis/keystone-init.git

<span class="co"># Replace the default configuration with the values defined be the</span>
<span class="co"># environmental variables in configrc</span>
<span class="kw">sed</span> -i <span class="st">&quot;s/192.168.206.130/controller/g&quot;</span> <span class="kw">\</span>
   keystone-init/config.yaml
<span class="kw">sed</span> -i <span class="st">&quot;s/012345SECRET99TOKEN012345/</span><span class="kw">`cat</span> keystone-admin-token<span class="kw">`</span><span class="st">/g&quot;</span> <span class="kw">\</span>
   keystone-init/config.yaml
<span class="kw">sed</span> -i <span class="st">&quot;s/name:        openstackDemo/name:        </span><span class="ot">$OS_TENANT_NAME</span><span class="st">/g&quot;</span> <span class="kw">\</span>
   keystone-init/config.yaml
<span class="kw">sed</span> -i <span class="st">&quot;s/name:     adminUser/name:     </span><span class="ot">$OS_USERNAME</span><span class="st">/g&quot;</span> <span class="kw">\</span>
   keystone-init/config.yaml
<span class="kw">sed</span> -i <span class="st">&quot;s/password: secretword/password: </span><span class="ot">$OS_PASSWORD</span><span class="st">/g&quot;</span> <span class="kw">\</span>
   keystone-init/config.yaml
<span class="kw">sed</span> -i <span class="st">&quot;s/name:     glance/name:     </span><span class="ot">$GLANCE_SERVICE_USERNAME</span><span class="st">/g&quot;</span> <span class="kw">\</span>
   keystone-init/config.yaml
<span class="kw">sed</span> -i <span class="st">&quot;s/password: glance/password: </span><span class="ot">$GLANCE_SERVICE_PASSWORD</span><span class="st">/g&quot;</span> <span class="kw">\</span>
   keystone-init/config.yaml
<span class="kw">sed</span> -i <span class="st">&quot;s/name:     nova/name:     </span><span class="ot">$NOVA_SERVICE_USERNAME</span><span class="st">/g&quot;</span> <span class="kw">\</span>
   keystone-init/config.yaml
<span class="kw">sed</span> -i <span class="st">&quot;s/password: nova/password: </span><span class="ot">$NOVA_SERVICE_PASSWORD</span><span class="st">/g&quot;</span> <span class="kw">\</span>
   keystone-init/config.yaml
<span class="kw">sed</span> -i <span class="st">&quot;s/RegionOne/</span><span class="ot">$OS_REGION_NAME</span><span class="st">/g&quot;</span> <span class="kw">\</span>
   keystone-init/config.yaml

<span class="co"># Run the Keystone initialization script</span>
./keystone-init/keystone-init.py ./keystone-init/config.yaml

<span class="kw">echo</span> <span class="st">&quot;&quot;</span>
<span class="kw">echo</span> <span class="st">&quot;The applied config file is keystone-init/config.yaml&quot;</span>
<span class="kw">echo</span> <span class="st">&quot;You may do &#39;rm -rf keystone-init&#39; to remove \</span>
<span class="st">   the no more needed keystone-init directory&quot;</span></code></pre>
<ol start="31" style="list-style-type: example">
<li><code>12-glance-install.sh</code></li>
</ol>
<p>This script install Glance -- the OpenStack VM image management service.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Install OpenStack Glance, an image management service</span>
yum <span class="kw">install</span> -y openstack-glance</code></pre>
<ol start="32" style="list-style-type: example">
<li><code>13-glance-create-db.sh</code></li>
</ol>
<p>This script creates a MySQL database for Glance called <code>glance</code>, which is used to store VM image metadata. The script also creates a <code>glance</code> user and grants full permissions to the <code>glance</code> database to this user.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Create a database for Glance</span>
../lib/mysqlq.sh <span class="st">&quot;CREATE DATABASE glance;&quot;</span>

<span class="co"># Create a glance user and grant all privileges</span>
<span class="co"># to the glance database</span>
../lib/mysqlq.sh <span class="st">&quot;GRANT ALL ON glance.* TO &#39;glance&#39;@&#39;controller&#39; \</span>
<span class="st">   IDENTIFIED BY &#39;</span><span class="ot">$GLANCE_MYSQL_PASSWORD</span><span class="st">&#39;;&quot;</span></code></pre>
<ol start="33" style="list-style-type: example">
<li><code>14-glance-config.sh</code></li>
</ol>
<p>This scripts modifies the configuration files of the Glance services, which include the API and Registry services. Apart from various credentials, the script also sets Keystone as the identity management service used by Glance.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Make Glance API use Keystone as the identity management service</span>
openstack-config --set /etc/glance/glance-api.conf <span class="kw">\</span>
   paste_deploy flavor keystone

<span class="co"># Set Glance API user credentials</span>
openstack-config --set /etc/glance/glance-api-paste.ini <span class="kw">\</span>
   filter:authtoken admin_tenant_name <span class="ot">$GLANCE_SERVICE_TENANT</span>
openstack-config --set /etc/glance/glance-api-paste.ini <span class="kw">\</span>
   filter:authtoken admin_user <span class="ot">$GLANCE_SERVICE_USERNAME</span>
openstack-config --set /etc/glance/glance-api-paste.ini <span class="kw">\</span>
   filter:authtoken admin_password <span class="ot">$GLANCE_SERVICE_PASSWORD</span>

<span class="co"># Set Glance Cache user credentials</span>
openstack-config --set /etc/glance/glance-cache.conf <span class="kw">\</span>
   DEFAULT admin_tenant_name <span class="ot">$GLANCE_SERVICE_TENANT</span>
openstack-config --set /etc/glance/glance-cache.conf <span class="kw">\</span>
   DEFAULT admin_user <span class="ot">$GLANCE_SERVICE_USERNAME</span>
openstack-config --set /etc/glance/glance-cache.conf <span class="kw">\</span>
   DEFAULT admin_password <span class="ot">$GLANCE_SERVICE_PASSWORD</span>

<span class="co"># Set Glance Registry to use Keystone</span>
<span class="co"># as the identity management service</span>
openstack-config --set /etc/glance/glance-registry.conf <span class="kw">\</span>
   paste_deploy flavor keystone

<span class="co"># Set the connection to the MySQL server</span>
openstack-config --set /etc/glance/glance-registry.conf <span class="kw">\</span>
   DEFAULT sql_connection <span class="kw">\</span>
      mysql://glance:<span class="ot">$GLANCE_MYSQL_PASSWORD</span>@controller/glance

<span class="co"># Set Glance Registry user credentials</span>
openstack-config --set /etc/glance/glance-registry-paste.ini <span class="kw">\</span>
   filter:authtoken admin_tenant_name <span class="ot">$GLANCE_SERVICE_TENANT</span>
openstack-config --set /etc/glance/glance-registry-paste.ini <span class="kw">\</span>
   filter:authtoken admin_user <span class="ot">$GLANCE_SERVICE_USERNAME</span>
openstack-config --set /etc/glance/glance-registry-paste.ini <span class="kw">\</span>
   filter:authtoken admin_password <span class="ot">$GLANCE_SERVICE_PASSWORD</span></code></pre>
<ol start="34" style="list-style-type: example">
<li><code>15-glance-init-db.sh</code></li>
</ol>
<p>This scripts initializes the <code>glance</code> database using the <code>glance-manage</code> command line tool.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Initialize the database for Glance</span>
glance-manage db_sync</code></pre>
<ol start="35" style="list-style-type: example">
<li><code>16-glance-permissions.sh</code></li>
</ol>
<p>This scripts sets restrictive permissions (640) on the Glance configuration files, since they contain sensitive information. The script also set the ownership of the Glance related directories to the <code>glance</code> user and <code>glance</code> group.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Set restrictive permissions for the Glance config files</span>
<span class="kw">chmod</span> 640 /etc/glance/*.conf
<span class="kw">chmod</span> 640 /etc/glance/*.ini

<span class="co"># Set the ownership for the Glance related directories</span>
<span class="kw">chown</span> -R glance:glance /var/log/glance
<span class="kw">chown</span> -R glance:glance /var/lib/glance</code></pre>
<ol start="36" style="list-style-type: example">
<li><code>17-glance-start.sh</code></li>
</ol>
<p>This script starts the Glance services: API and Registry. The script sets the services to automatically start during the system start up.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Start the Glance Registry and API services</span>
service openstack-glance-registry restart
service openstack-glance-api restart

chkconfig openstack-glance-registry on
chkconfig openstack-glance-api on</code></pre>
<ol start="37" style="list-style-type: example">
<li><code>18-add-cirros.sh</code></li>
</ol>
<p>This script downloads the CirrOS VM image<sup><a href="#fn24" class="footnoteRef" id="fnref24">24</a></sup> and imports it into Glance. This image contains a pre-installed CirrOS, a Tiny OS specialized for running in a Cloud. The image is very simplistic: its size is just 9.4 MB. However, it is sufficient for testing OpenStack.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Download the CirrOS VM image</span>
<span class="kw">mkdir</span> /tmp/images
<span class="kw">cd</span> /tmp/images
<span class="kw">wget</span> https://launchpad.net/cirros/trunk/0.3.0/+download/<span class="kw">\</span>
   cirros-0.3.0-x86_64-disk.img

<span class="co"># Add the downloaded image to Glance</span>
glance add <span class="ot">name=</span><span class="st">&quot;cirros-0.3.0-x86_64&quot;</span> is_public=true <span class="kw">\</span>
   <span class="ot">disk_format=</span>qcow2 container_format=bare <span class="kw">\</span>
   <span class="kw">&lt;</span> cirros-0.3.0-x86_64-disk.img

<span class="co"># Remove the temporary directory</span>
<span class="kw">rm</span> -rf /tmp/images</code></pre>
<ol start="38" style="list-style-type: example">
<li><code>19-add-ubuntu.sh</code></li>
</ol>
<p>This script downloads the Ubuntu Cloud Image<sup><a href="#fn25" class="footnoteRef" id="fnref25">25</a></sup> and imports it into Glance. This is a VM image with a pre-installed version of Ubuntu that is customized by Ubuntu engineering to run on Cloud platforms such as Openstack, Amazon EC2, and LXC.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Download an Ubuntu Cloud image</span>
<span class="kw">mkdir</span> /tmp/images
<span class="kw">cd</span> /tmp/images
<span class="kw">wget</span> http://uec-images.ubuntu.com/precise/current/<span class="kw">\</span>
   precise-server-cloudimg-amd64-disk1.img

<span class="co"># Add the downloaded image to Glance</span>
glance add <span class="ot">name=</span><span class="st">&quot;ubuntu&quot;</span> is_public=true disk_format=qcow2 <span class="kw">\</span>
   <span class="ot">container_format=</span>bare <span class="kw">&lt;</span> precise-server-cloudimg-amd64-disk1.img

<span class="co"># Remove the temporary directory</span>
<span class="kw">rm</span> -rf /tmp/images</code></pre>
<ol start="39" style="list-style-type: example">
<li><code>20-nova-install.sh</code></li>
</ol>
<p>This script installs Nova -- the OpenStack compute service, as well as the Qpid AMQP message broker. The message broker is required by the OpenStack services to communicate with each other.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Install OpenStack Nova (compute service)</span>
<span class="co"># and the Qpid AMQP message broker</span>
yum <span class="kw">install</span> -y openstack-nova* qpid-cpp-server</code></pre>
<ol start="40" style="list-style-type: example">
<li><code>21-nova-create-db.sh</code></li>
</ol>
<p>This script creates a MySQL database for Nova called <code>nova</code>, which is used to store VM instance metadata. The script also creates a <code>nova</code> user and grants full permissions to the <code>nova</code> database to this user. The script also enables the access to the database from hosts other than controller.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Create a database for Nova</span>
../lib/mysqlq.sh <span class="st">&quot;CREATE DATABASE nova;&quot;</span>

<span class="co"># Create a nova user and grant all privileges</span>
<span class="co"># to the nova database</span>
../lib/mysqlq.sh <span class="st">&quot;GRANT ALL ON nova.* TO &#39;nova&#39;@&#39;controller&#39; \</span>
<span class="st">   IDENTIFIED BY &#39;</span><span class="ot">$NOVA_MYSQL_PASSWORD</span><span class="st">&#39;;&quot;</span>

<span class="co"># The following is need to allow access</span>
<span class="co"># from Nova services running on other hosts</span>
../lib/mysqlq.sh <span class="st">&quot;GRANT ALL ON nova.* TO &#39;nova&#39;@&#39;%&#39; \</span>
<span class="st">   IDENTIFIED BY &#39;</span><span class="ot">$NOVA_MYSQL_PASSWORD</span><span class="st">&#39;;&quot;</span></code></pre>
<ol start="41" style="list-style-type: example">
<li><code>22-nova-permissions.sh</code></li>
</ol>
<p>This script sets restrictive permissions on the Nova configuration file, since it contains sensitive information, such as user credentials. The script also sets the ownership of the Nova related directories to the <code>nova</code> group.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Set restrictive permissions for the Nova config file</span>
<span class="kw">chmod</span> 640 /etc/nova/nova.conf

<span class="co"># Set the ownership for the Nova related directories</span>
<span class="kw">chown</span> -R root:nova /etc/nova
<span class="kw">chown</span> -R nova:nova /var/lib/nova</code></pre>
<ol start="42" style="list-style-type: example">
<li><code>23-nova-config.sh</code></li>
</ol>
<p>The <code>/etc/nova/nova.conf</code> configuration file should be present on all the compute hosts running Nova Compute, as well as on the controller, which runs the other Nova services. Moreover, the content of the configuration file should be the same on the controller and compute hosts. Therefore, a script that modifies the Nova configuration is placed in the <code>lib</code> directory and is shared by the corresponding installation scripts of the controller and compute hosts. The <code>23-nova-config.sh</code> script invokes the Nova configuration script provided in the <code>lib</code> directory.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Run the Nova configuration script</span>
<span class="co"># defined in ../lib/nova-config.sh</span>
../lib/nova-config.sh</code></pre>
<p>The content of the <code>nova-config.sh</code> script is given below:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># This is a Nova configuration shared</span>
<span class="co"># by the compute hosts, gateway and controller</span>

<span class="co"># Enable verbose output</span>
openstack-config --set /etc/nova/nova.conf <span class="kw">\</span>
   DEFAULT verbose True

<span class="co"># Set the connection to the MySQL server</span>
openstack-config --set /etc/nova/nova.conf <span class="kw">\</span>
   DEFAULT sql_connection <span class="kw">\</span>
      mysql://nova:<span class="ot">$NOVA_MYSQL_PASSWORD</span>@controller/nova

<span class="co"># Make Nova use Keystone as the identity management service</span>
openstack-config --set /etc/nova/nova.conf <span class="kw">\</span>
   DEFAULT auth_strategy keystone

<span class="co"># Set the host name of the Qpid AMQP message broker</span>
openstack-config --set /etc/nova/nova.conf <span class="kw">\</span>
   DEFAULT qpid_hostname controller

<span class="co"># Set Nova user credentials</span>
openstack-config --set /etc/nova/api-paste.ini <span class="kw">\</span>
   filter:authtoken admin_tenant_name <span class="ot">$NOVA_SERVICE_TENANT</span>
openstack-config --set /etc/nova/api-paste.ini <span class="kw">\</span>
   filter:authtoken admin_user <span class="ot">$NOVA_SERVICE_USERNAME</span>
openstack-config --set /etc/nova/api-paste.ini <span class="kw">\</span>
   filter:authtoken admin_password <span class="ot">$NOVA_SERVICE_PASSWORD</span>
openstack-config --set /etc/nova/api-paste.ini <span class="kw">\</span>
   filter:authtoken auth_uri <span class="ot">$NOVA_OS_AUTH_URL</span>

<span class="co"># Set the network configuration</span>
openstack-config --set /etc/nova/nova.conf <span class="kw">\</span>
   DEFAULT network_host compute1
openstack-config --set /etc/nova/nova.conf <span class="kw">\</span>
   DEFAULT fixed_range 10.0.0.0/24
openstack-config --set /etc/nova/nova.conf <span class="kw">\</span>
   DEFAULT flat_interface eth1
openstack-config --set /etc/nova/nova.conf <span class="kw">\</span>
   DEFAULT flat_network_bridge br100
openstack-config --set /etc/nova/nova.conf <span class="kw">\</span>
   DEFAULT public_interface eth1

<span class="co"># Set the Glance host name</span>
openstack-config --set /etc/nova/nova.conf <span class="kw">\</span>
   DEFAULT glance_host controller

<span class="co"># Set the VNC configuration</span>
openstack-config --set /etc/nova/nova.conf <span class="kw">\</span>
   DEFAULT vncserver_listen 0.0.0.0
openstack-config --set /etc/nova/nova.conf <span class="kw">\</span>
   DEFAULT vncserver_proxyclient_address controller

<span class="co"># This is the host accessible from outside,</span>
<span class="co"># where novncproxy is running on</span>
openstack-config --set /etc/nova/nova.conf <span class="kw">\</span>
   DEFAULT novncproxy_base_url <span class="kw">\</span>
      http://<span class="ot">$PUBLIC_IP_ADDRESS</span>:6080/vnc_auto.html

<span class="co"># This is the host accessible from outside,</span>
<span class="co"># where xvpvncproxy is running on</span>
openstack-config --set /etc/nova/nova.conf <span class="kw">\</span>
   DEFAULT xvpvncproxy_base_url <span class="kw">\</span>
      http://<span class="ot">$PUBLIC_IP_ADDRESS</span>:6081/console

<span class="co"># Set the host name of the metadata service</span>
openstack-config --set /etc/nova/nova.conf <span class="kw">\</span>
   DEFAULT metadata_host <span class="ot">$METADATA_HOST</span></code></pre>
<p>Apart from user credentials, the script configures a few other important options:</p>
<ul>
<li>the identity management service -- Keystone;</li>
<li>the Qpid server host name -- controller;</li>
<li>the host running the Nova network service -- compute1 (i.e. gateway);</li>
<li>the network used for VMs -- 10.0.0.0/24;</li>
<li>the network interface used to bridge VMs to -- eth1;</li>
<li>the Linux bridge used by VMs -- br100;</li>
<li>the public network interface -- eth1;</li>
<li>the Glance service host name -- controller;</li>
<li>the VNC server host name -- controller;</li>
<li>the IP address of the host running VNC proxies (they must be run on the host that can be accessed from outside; in our setup it is the gateway) -- <code>$PUBLIC_IP_ADDRESS</code>;</li>
<li>the Nova metadata service host name -- controller.</li>
</ul>
<ol start="43" style="list-style-type: example">
<li><code>24-nova-init-db.sh</code></li>
</ol>
<p>This scripts initializes the <code>nova</code> database using the <code>nova-manage</code> command line tool.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Initialize the database for Nova</span>
nova-manage db <span class="kw">sync</span></code></pre>
<ol start="44" style="list-style-type: example">
<li><code>25-nova-start.sh</code></li>
</ol>
<p>This script starts various Nova services, as well as their dependencies: the Qpid AMQP message broker, and iSCSI target daemon used by the <code>nova-volume</code> service.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Start the Qpid AMQP message broker</span>
service qpidd restart

<span class="co"># iSCSI target daemon for nova-volume</span>
service tgtd restart

<span class="co"># Start OpenStack Nova services</span>
service openstack-nova-api restart
service openstack-nova-cert restart
service openstack-nova-consoleauth restart
service openstack-nova-direct-api restart
service openstack-nova-metadata-api restart
service openstack-nova-scheduler restart
service openstack-nova-volume restart

<span class="co"># Make the service start on the system startup</span>
chkconfig qpidd on
chkconfig tgtd on
chkconfig openstack-nova-api on
chkconfig openstack-nova-cert on
chkconfig openstack-nova-consoleauth on
chkconfig openstack-nova-direct-api on
chkconfig openstack-nova-metadata-api on
chkconfig openstack-nova-scheduler on
chkconfig openstack-nova-volume on</code></pre>
<h4 id="openstack-compute-compute-nodes.">08-openstack-compute (compute nodes).</h4>
<p>The scripts described in this section should be run on the compute hosts.</p>
<ol start="45" style="list-style-type: example">
<li><code>01-source-configrc.sh</code></li>
</ol>
<p>This scripts is mainly used to remind of the necessity to &quot;source&quot; the <code>configrc</code> file prior to continuing, since some scripts in this directory use the environmental variable defined in <code>configrc</code>. To source the file, it is necessary to run the following command: <code>. 01-source-configrc.sh</code>.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="kw">echo</span> <span class="st">&quot;To make the environmental variables available \</span>
<span class="st">   in the current session, run: &quot;</span>
<span class="kw">echo</span> <span class="st">&quot;. 01-source-configrc.sh&quot;</span>

<span class="co"># Export the variables defined in ../config/configrc</span>
<span class="kw">.</span> ../config/configrc</code></pre>
<ol start="46" style="list-style-type: example">
<li><code>02-install-nova.sh</code></li>
</ol>
<p>This script installs OpenStack Nova and OpenStack utilities.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Install OpenStack Nova and utils</span>
yum <span class="kw">install</span> -y openstack-nova* openstack-utils</code></pre>
<ol start="47" style="list-style-type: example">
<li><code>03-nova-permissions.sh</code></li>
</ol>
<p>This script sets restrictive permissions (640) on the Nova configuration file, since it contains sensitive information, such as user credentials. Then, the script sets the ownership on the Nova and Libvirt related directories to the <code>nova</code> user and <code>nova</code> group. The script also sets the user and group used by the Quick EMUlator<sup><a href="#fn26" class="footnoteRef" id="fnref26">26</a></sup> (QEMU) service to <code>nova</code>. This is required since a number of directories need to accessed by both Nova using the <code>nova</code> user and <code>nova</code> group, and QEMU.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Set restrictive permissions for the Nova config file</span>
<span class="kw">chmod</span> 640 /etc/nova/nova.conf

<span class="co"># Set the ownership for the Nova related directories</span>
<span class="kw">chown</span> -R root:nova /etc/nova
<span class="kw">chown</span> -R nova:nova /var/lib/nova
<span class="kw">chown</span> -R nova:nova /var/cache/libvirt
<span class="kw">chown</span> -R nova:nova /var/run/libvirt
<span class="kw">chown</span> -R nova:nova /var/lib/libvirt

<span class="co"># Make Qemu run under the nova user and group</span>
<span class="kw">sed</span> -i <span class="st">&#39;s/#user = &quot;root&quot;/user = &quot;nova&quot;/g&#39;</span> /etc/libvirt/qemu.conf
<span class="kw">sed</span> -i <span class="st">&#39;s/#group = &quot;root&quot;/group = &quot;nova&quot;/g&#39;</span> /etc/libvirt/qemu.conf</code></pre>
<ol start="48" style="list-style-type: example">
<li><code>04-nova-config.sh</code></li>
</ol>
<p>This scripts invokes the Nova configuration script provided in the <code>lib</code> directory, which has been detailed above.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Run the Nova configuration script</span>
<span class="co"># defined in ../lib/nova-config.sh</span>
../lib/nova-config.sh</code></pre>
<ol start="49" style="list-style-type: example">
<li><code>05-nova-compute-start.sh</code></li>
</ol>
<p>First, this script restarts the Libvirt service since its configuration has been modified. Then, the script starts Nova compute service and sets it to automatically start during the system start up.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Start the Libvirt and Nova services</span>
service libvirtd restart
service openstack-nova-compute restart
chkconfig openstack-nova-compute on</code></pre>
<h4 id="openstack-gateway-network-gateway.">09-openstack-gateway (network gateway).</h4>
<p>The scripts described in this section need to be run only on the gateway.</p>
<p>Nova supports three network configuration modes:</p>
<ol style="list-style-type: decimal">
<li><p>Flat Mode: public IP addresses from a specified range are assigned and injected into VM instances on launch. This only works on Linux systems that keep their network configuration in <code>/etc/network/interfaces</code>. To enable this mode, the following option should be specified in <code>nova.conf</code>:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="ot">network_manager=</span>nova.network.manager.FlatManager</code></pre></li>
<li><p>Flat DHCP Mode: Nova runs a Dnsmasq<sup><a href="#fn27" class="footnoteRef" id="fnref27">27</a></sup> server listening to a created network bridge that assigns public IP addresses to VM instances. This is the mode we use in this work. There must be only one host running the <code>openstack-nova-network</code> service. The <code>network_host</code> option in <code>nova.conf</code> specifies which host the <code>openstack-nova-network</code> service is running on. The network bridge name is specified using the <code>flat_network_bridge</code> option. To enable this mode, the following option should be specified in <code>nova.conf</code>:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="ot">network_manager=</span>nova.network.manager.FlatDHCPManager</code></pre></li>
<li><p>VLAN Mode: VM instances are assigned private IP addresses from networks created for each tenant / project. Instances are accessed through a special VPN VM instance. To enable this mode, the following option should be specified in <code>nova.conf</code>:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="ot">network_manager=</span>nova.network.manager.VlanManager</code></pre></li>
</ol>
<p>Nova runs a metadata service on http://169.254.169.254 that is queried by VM instances to obtain SSH keys and other user data. The <code>openstack-nova-network</code> service automatically configures <code>iptables</code> to NAT the port 80 of 169.254.169.254 to the IP address specified in the <code>metadata_host</code> option and the port specified in the <code>metadata_port</code> option configured in <code>nova.conf</code> (the defaults are the IP address of the <code>openstack-nova-network</code> service and 8775). If the <code>openstack-nova-metadata-api</code> and <code>openstack-nova-network</code> services are running on different hosts, the <code>metadata_host</code> option should point to the IP address of <code>openstack-nova-metadata-api</code>.</p>
<ol start="50" style="list-style-type: example">
<li><code>01-source-configrc.sh</code></li>
</ol>
<p>This scripts is mainly used to remind of the necessity to &quot;source&quot; the <code>configrc</code> file prior to continuing, since some scripts in this directory use the environmental variable defined in <code>configrc</code>. To source the file, it is necessary to run the following command: <code>. 01-source-configrc.sh</code>.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="kw">echo</span> <span class="st">&quot;To make the environmental variables available \</span>
<span class="st">   in the current session, run: &quot;</span>
<span class="kw">echo</span> <span class="st">&quot;. 01-source-configrc.sh&quot;</span>

<span class="co"># Export the variables defined in ../config/configrc</span>
<span class="kw">.</span> ../config/configrc</code></pre>
<ol start="51" style="list-style-type: example">
<li><code>02-nova-start.sh</code></li>
</ol>
<p>It is assumed that the gateway host is one of the compute hosts; therefore, the OpenStack compute service has already been configured and is running. This scripts starts 3 additional Nova services that are specific to the gateway host: <code>openstack-nova-network</code>, <code>openstack-nova-novncproxy</code>, and <code>openstack-nova-xvpvncproxy</code>. The <code>openstack-nova-network</code> service is responsible for bridging VM instances into the physical network, and configuring the Dnsmasq service for assigning IP addresses to the VMs. The VNC proxy services enable VNC connections to VM instances from the outside network; therefore, they must be run on a machine that has access to the public network, which is the gateway in our case.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Start the Libvirt and Nova services</span>
<span class="co"># (network, compute and VNC proxies)</span>
service libvirtd restart
service openstack-nova-network restart
service openstack-nova-compute restart
service openstack-nova-novncproxy restart
service openstack-nova-xvpvncproxy restart

<span class="co"># Make the service start on the system start up</span>
chkconfig openstack-nova-network on
chkconfig openstack-nova-compute on
chkconfig openstack-nova-novncproxy on
chkconfig openstack-nova-xvpvncproxy on</code></pre>
<ol start="52" style="list-style-type: example">
<li><code>03-nova-network-create.sh</code></li>
</ol>
<p>This service creates a Nova network 10.0.0.0/24, which is used to allocate IP addresses from by Dnsmasq to VM instances. The created network is configured to use the <code>br100</code> Linux bridge to connect VM instances to the physical network.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Create a Nova network for VM instances: 10.0.0.0/24</span>
nova-manage network create --label=public <span class="kw">\</span>
   --fixed_range_v4=10.0.0.0/24 --num_networks=1 <span class="kw">\</span>
   --network_size=256 --bridge=br100</code></pre>
<ol start="53" style="list-style-type: example">
<li><code>04-nova-secgroup-add.sh</code></li>
</ol>
<p>This script adds two rules to the default OpenStack security group. The first rule enables the Internet Control Message Protocol (ICMP) for VM instances (the ping command). The second rule enables TCP connections via the 22 port, which is used by SSH.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Enable ping for VMs</span>
nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0

<span class="co"># Enable SSH for VMs</span>
nova secgroup-add-rule default tcp 22 22 0.0.0.0/0</code></pre>
<ol start="54" style="list-style-type: example">
<li><code>05-dashboard-install.sh</code></li>
</ol>
<p>This script installs the OpenStack dashboard. The OpenStack dashboard provides a web-interface to managing an OpenStack environment. Since the dashboard is supposed to be accessed from outside, this service must be installed on a host that has access to the public network, which is the gateway in our setup.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Install OpenStack Dashboard</span>
yum <span class="kw">install</span> -y openstack-dashboard</code></pre>
<ol start="55" style="list-style-type: example">
<li><code>06-dashboard-config.sh</code></li>
</ol>
<p>This script configures the OpenStack dashboard. Particularly, the script sets the <code>OPENSTACK_HOST</code> configuration option denoting the host name of the management host to <code>controller</code>. The script also sets the default Keystone role to the value of the <code>$OS_TENANT_NAME</code> environmental variable.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Set the OpenStack management host</span>
<span class="kw">sed</span> -i <span class="st">&#39;s/OPENSTACK_HOST = &quot;127.0.0.1&quot;/\</span>
<span class="st">   OPENSTACK_HOST = &quot;controller&quot;/g&#39;</span> <span class="kw">\</span>
   /etc/openstack-dashboard/local_settings

<span class="co"># Set the Keystone default role</span>
<span class="kw">sed</span> -i <span class="st">&quot;s/OPENSTACK_KEYSTONE_DEFAULT_ROLE = </span><span class="dt">\&quot;</span><span class="st">Member</span><span class="dt">\&quot;</span><span class="st">/\</span>
<span class="st">   OPENSTACK_KEYSTONE_DEFAULT_ROLE = </span><span class="dt">\&quot;</span><span class="ot">$OS_TENANT_NAME</span><span class="dt">\&quot;</span><span class="st">/g&quot;</span> <span class="kw">\</span>
   /etc/openstack-dashboard/local_settings</code></pre>
<ol start="56" style="list-style-type: example">
<li><code>07-dashboard-start.sh</code></li>
</ol>
<p>This script starts the <code>httpd</code> service, which is a web server configured to serve the OpenStack dashboard. The script also sets the <code>httpd</code> service to start automatically during the system start up. Once the service is started, the dashboard will be available at <code>http://localhost/dashboard</code>, where 'localhost' should be replaced by the public IP address of the gateway host for accessing the dashboard from the outside network.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Start the httpd service.</span>
service httpd restart
chkconfig httpd on</code></pre>
<p>At this point the installation of OpenStack can be considered completed. The next steps are only intended for testing the environment.</p>
<h4 id="openstack-controller-controller.-1">10-openstack-controller (controller).</h4>
<p>This section describes commands and scripts that can be used to test the OpenStack installation obtained by following the steps above. The testing should start from the identity management service, Keystone, since it coordinates all the other OpenStack services. To use the command line programs provided by OpenStack, it is necessary to &quot;source&quot; the <code>configrc</code>. This can be done by executing the following command: <code>. config/configrc</code>. The check whether Keystone is properly initialized and the authorization works, the following command can be used:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash">keystone user-list</code></pre>
<p>If everything is configured correctly, the command should output a table with a list of user accounts, such as <code>admin</code>, <code>nova</code>, <code>glance</code>, etc.</p>
<p>The next service to test is Glance. In the previous steps, we have already imported VM images into Glance; therefore, it is possible to output a list of them:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash">glance index</code></pre>
<p>The command should output a list of two VM images: <code>cirros-0.3.0-x86_64</code> and <code>ubuntu</code>.</p>
<p>A list of active OpenStack service spanning all the hosts can be output using the following command:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash">nova-manage service list</code></pre>
<p>The command should output approximately the following table:</p>
<table>
<caption>The expected output of the <code>nova-manage service list</code> command</caption>
<col width="23%" />
<col width="15%" />
<col width="6%" />
<col width="11%" />
<col width="8%" />
<col width="12%" />
<thead>
<tr class="header">
<th align="left">Binary</th>
<th align="left">Host</th>
<th align="left">Zone</th>
<th align="left">Status</th>
<th align="left">State</th>
<th align="left">Updated</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">nova-consoleauth</td>
<td align="left">controller</td>
<td align="left">nova</td>
<td align="left">enabled</td>
<td align="left">:-)</td>
<td align="left">&lt;date&gt;</td>
</tr>
<tr class="even">
<td align="left">nova-cert</td>
<td align="left">controller</td>
<td align="left">nova</td>
<td align="left">enabled</td>
<td align="left">:-)</td>
<td align="left">&lt;date&gt;</td>
</tr>
<tr class="odd">
<td align="left">nova-scheduler</td>
<td align="left">controller</td>
<td align="left">nova</td>
<td align="left">enabled</td>
<td align="left">:-)</td>
<td align="left">&lt;date&gt;</td>
</tr>
<tr class="even">
<td align="left">nova-volume</td>
<td align="left">controller</td>
<td align="left">nova</td>
<td align="left">enabled</td>
<td align="left">:-)</td>
<td align="left">&lt;date&gt;</td>
</tr>
<tr class="odd">
<td align="left">nova-compute</td>
<td align="left">compute1</td>
<td align="left">nova</td>
<td align="left">enabled</td>
<td align="left">:-)</td>
<td align="left">&lt;date&gt;</td>
</tr>
<tr class="even">
<td align="left">nova-compute</td>
<td align="left">compute2</td>
<td align="left">nova</td>
<td align="left">enabled</td>
<td align="left">:-)</td>
<td align="left">&lt;date&gt;</td>
</tr>
<tr class="odd">
<td align="left">nova-compute</td>
<td align="left">compute3</td>
<td align="left">nova</td>
<td align="left">enabled</td>
<td align="left">:-)</td>
<td align="left">&lt;date&gt;</td>
</tr>
<tr class="even">
<td align="left">nova-compute</td>
<td align="left">compute4</td>
<td align="left">nova</td>
<td align="left">enabled</td>
<td align="left">:-)</td>
<td align="left">&lt;date&gt;</td>
</tr>
<tr class="odd">
<td align="left">nova-network</td>
<td align="left">controller</td>
<td align="left">nova</td>
<td align="left">enabled</td>
<td align="left">:-)</td>
<td align="left">&lt;date&gt;</td>
</tr>
</tbody>
</table>
<p>If the value of any cell in the <code>State</code> column is <code>XXX</code> instead of <code>:-)</code>, it means that the corresponding service failed to start. The first place to start troubleshooting is the log files of the failed service. The log files are located in the <code>/var/log/&lt;service&gt;</code> directory, where <code>&lt;service&gt;</code> is replaced with the name of the service.</p>
<p>Another service to test is the OpenStack dashboard, which should be available at <code>http://$PUBLIC_IP_ADDRESS/dashboard</code>. This URL should open a login page prompting the user to enter a user name and password. The values of the <code>$OS_USERNAME</code> and <code>$OS_PASSWORD</code> variables defined in <code>configrc</code> can be used to log in as the admin user. The dashboard provides a web interface to all the main functionality of OpenStack, such as managing VM instances, VM images, security rules, key pairs, etc.</p>
<p>Once the initial testing steps are successfully passed, we can go on to test the actual instantiation of VMs using the OpenStack command line tools, as shown by the scripts from the <code>10-openstack-controller</code> directory.</p>
<ol start="57" style="list-style-type: example">
<li><code>01-source-configrc.sh</code></li>
</ol>
<p>This scripts is mainly used to remind of the necessity to &quot;source&quot; the <code>configrc</code> file prior to continuing, since some scripts in this directory use the environmental variable defined in <code>configrc</code>. To source the file, it is necessary to run the following command: <code>. 01-source-configrc.sh</code>.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="kw">echo</span> <span class="st">&quot;To make the environmental variables available \</span>
<span class="st">   in the current session, run: &quot;</span>
<span class="kw">echo</span> <span class="st">&quot;. 01-source-configrc.sh&quot;</span>

<span class="co"># Export the variables defined in ../config/configrc</span>
<span class="kw">.</span> ../config/configrc</code></pre>
<ol start="58" style="list-style-type: example">
<li><code>02-boot-cirros.sh</code></li>
</ol>
<p>This script creates a VM instance using the CirrOS image added to Glance previously.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Create a VM instance from the CirrOS image</span>
nova boot --image cirros-0.3.0-x86_64 --flavor m1.small cirros</code></pre>
<p>Depending on the hardware the instantiation process may take from a few seconds to a few minutes. The status of a VM instance can be checked using the following command:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash">nova show cirros</code></pre>
<p>This command shows detailed information about the VM instances, such as the host name, where the VM has been allocated to, instance name, current state, flavor, image name, IP address of the VM, etc. Once the state of the VM turns into <code>ACTIVE</code>, it means that the VM has started booting. It may take some more time before the VM is ready to accept SSH connections. The CirrOS VM image has a default user <code>cirros</code> with the <code>cubswin:)</code> password. The following command can be used to SSH into the VM instance once it is booted:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="kw">ssh</span> curros@<span class="kw">&lt;</span>ip address<span class="kw">&gt;</span></code></pre>
<p>Where <code>&lt;ip address&gt;</code> is replaced with the actual IP address of the VM instance. The following command can be used to delete the VM instance:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash">nova delete cirros</code></pre>
<ol start="59" style="list-style-type: example">
<li><code>03-keypair-add.sh</code></li>
</ol>
<p>Nova supports injection of SSH keys into VM instances for password-less authentication. This script creates a key pair, which can be used by Nova to inject into VMs. The generated public key is stored internally by Nova, whereas, the private key is saved into the specified <code>../config/test.pem</code> file.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Create a key pair</span>
nova keypair-add <span class="kw">test</span> <span class="kw">&gt;</span> ../config/test.pem
<span class="kw">chmod</span> 600 ../config/test.pem</code></pre>
<ol start="60" style="list-style-type: example">
<li><code>04-boot-ubuntu.sh</code></li>
</ol>
<p>This script creates a VM instance using the Ubuntu Cloud image added to Glance previously. The executed command instructs OpenStack to inject the previously generated public key called <code>test</code> to allow password-less SSH connections.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Create a VM instance from the Ubuntu Cloud image</span>
nova boot --image ubuntu --flavor m1.small --key_name <span class="kw">test</span> ubuntu</code></pre>
<ol start="61" style="list-style-type: example">
<li><code>05-ssh-into-vm.sh</code></li>
</ol>
<p>This script shows how to SSH into a VM instance, which has been injected with the previously generated <code>test</code> key. The script accepts one argument: the IP address of the VM instance.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># SSH into a VM instance using the generated test.pem key.</span>

<span class="kw">if </span>[ <span class="ot">$#</span> -ne 1 ]
<span class="kw">then</span>
    <span class="kw">echo</span> <span class="st">&quot;You must specify one arguments - \</span>
<span class="st">       the IP address of the VM instance&quot;</span>
    <span class="kw">exit</span> 1
<span class="kw">fi</span>

<span class="kw">ssh</span> -i ../config/test.pem -l <span class="kw">test</span> <span class="ot">$1</span></code></pre>
<ol start="62" style="list-style-type: example">
<li><code>06-nova-volume-create.sh</code></li>
</ol>
<p>This script shows how to create a 2 GB Nova volume called <code>myvolume</code>. Once created, the volume can be dynamically attached to a VM instance, as shown in the next script. A volume can only be attached to one instance at a time.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Create a 2GB volume called myvolume</span>
nova volume-create --display_name myvolume 2</code></pre>
<ol start="63" style="list-style-type: example">
<li><code>07-nova-volume-attach.sh</code></li>
</ol>
<p>This script shows how to attached a volume to a VM instance. The script accepts two arguments: (1) the name of the VM instance to attach the volume to; and (2) the ID of the volume to attach to the VM instance. Once attached, the volume will be available inside the VM instance as the <code>/dev/vdc/</code> device. The volume is provided as a block storage, which means it has be formatted before it can be used.</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="co"># Attach the created volume to a VM instance as /dev/vdc.</span>

<span class="kw">if </span>[ <span class="ot">$#</span> -ne 2 ]
<span class="kw">then</span>
    <span class="kw">echo</span> <span class="st">&quot;You must specify two arguments:&quot;</span>
    <span class="kw">echo</span> <span class="st">&quot;(1) the name of the VM instance&quot;</span>
    <span class="kw">echo</span> <span class="st">&quot;(2) the ID of the volume to attach&quot;</span>
    <span class="kw">exit</span> 1
<span class="kw">fi</span>

nova volume-attach <span class="ot">$1</span> <span class="ot">$2</span> /dev/vdc</code></pre>
<h2 id="openstack-troubleshooting">OpenStack Troubleshooting</h2>
<p>This section lists some of the problems encountered by the authors during the installation process and their solutions. The following general procedure can be used to resolve problems with OpenStack:</p>
<ol style="list-style-type: decimal">
<li>Run the <code>nova-manage service list</code> command to find out if any of the services failed. A service failed if the corresponding row of the table the <code>State</code> column contains <code>XXX</code> instead of <code>:-)</code>.</li>
<li>From the same service status table, the host running the failed service can be identified by looking at the <code>Host</code> column.</li>
<li>Once the problematic service and host are determined, the respective log files should be examined. To do this, it is necessary to open an SSH connection with the host and find the log file that corresponds to the failed service. The default location of the log files is <code>/var/log/&lt;service name&gt;</code>, where <code>&lt;service name&gt;</code> is one of: <code>keystone</code>, <code>glance</code>, <code>nova</code>, etc.</li>
</ol>
<h3 id="glance">Glance</h3>
<p>Sometimes the Glance Registry service fails to start during the OS start up. This results in failing of various requests of the OpenStack services to Glance. The problem can be identified by running the <code>glance index</code> command, which should not fail in a normal case. The reason of a failure might be the fact that the Glance Registry service starts before the MySQL server. The solution to this problem is to restart the Glance services as follows:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash">service openstack-glance-registry restart
service openstack-glance-api restart</code></pre>
<h3 id="nova-compute">Nova Compute</h3>
<p>The <code>libvirtd</code> service may fail with errors, such the following:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash">15391: error <span class="kw">:</span> qemuProcessReadLogOutput:1005 <span class="kw">:</span> <span class="kw">\</span>
   internal error Process exited <span class="kw">while</span> reading console <span class="kw">\</span>
   log output: chardev: opening backend <span class="st">&quot;file&quot;</span> failed</code></pre>
<p>And such as:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash">error <span class="kw">:</span> qemuProcessReadLogOutput:1005 <span class="kw">:</span> internal error <span class="kw">\</span>
   Process exited <span class="kw">while</span> reading console log output: <span class="kw">\</span>
   char device redirected to /dev/pts/3
qemu-kvm: -drive <span class="ot">file=</span>/var/lib/nova/instances/instance-00000015/ <span class="kw">\</span>
   disk,<span class="ot">if=</span>none,id=drive-virtio-disk0,format=qcow2,cache=none: <span class="kw">\</span>
   could not open disk image /var/lib/nova/instances/ <span class="kw">\</span>
   instance-00000015/disk: Permission denied</code></pre>
<p>Both the problems can be resolved by setting the user and group in the <code>/etc/libvirt/libvirtd.conf</code> configuration file as follows:</p>
<pre><code>user = &quot;nova&quot;
group = &quot;nova&quot;</code></pre>
<p>And also changing the ownership as follows:</p>
<pre><code>chown -R nova:nova /var/cache/libvirt
chown -R nova:nova /var/run/libvirt
chown -R nova:nova /var/lib/libvirt</code></pre>
<h3 id="nova-network">Nova Network</h3>
<p>If after a start up, the <code>openstack-nova-network</code> service hangs with the following last message in the log file: 'Attempting to grab file lock &quot;iptables&quot; for method &quot;apply&quot;', the solution is the following<sup><a href="#fn28" class="footnoteRef" id="fnref28">28</a></sup>:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="kw">rm</span> /var/lib/nova/tmp/nova-iptables.lock</code></pre>
<h1 id="conclusion">Conclusion</h1>
<p>We have gone through and discussed all the steps required to get from bare hardware to a fully operating OpenStack infrastructure. We have started from notes on installing CentOS on the nodes, continued through setting up a network gateway, distributed replicated storage using GlusterFS, KVM hypervisor, and all the main OpenStack services. We have concluded with steps to test the OpenStack installation, suggestions on ways of finding problem sources and resolving them, and a discussion of solutions to a number of problems that may be encountered during the installation process.</p>
<p>In our opinion, the availability of step-by-step installation and configuration guides, such as this one, is very important to lower the barrier to entry into the real world application of open source Cloud platforms for a wider audience. The task of providing such a guidance lies on both the official documentation and tutorials and materials developed by the project community. It is hard to underestimate the role of the community support in facilitating the adoption of open source software. We believe that the OpenStack project has attracted a large, active and growing community of people, who will undoubtedly greatly contribute to further advancement of both the software and documentation of OpenStack leading to a significant impact on the adoption of free open source software and Cloud computing.</p>
<h1 id="references">References</h1>
<p>[1] M. Armbrust, A. Fox, R. Griffith, A. D. Joseph, R. Katz, A. Konwinski, G. Lee, D. Patterson, A. Rabkin, I. Stoica, and others, “A view of cloud computing,” <em>Communications of the ACM</em>, vol. 53, pp. 50–58, 2010.</p>
<p>[2] R. Buyya, C. S. Yeo, S. Venugopal, J. Broberg, and I. Brandic, “Cloud computing and emerging IT platforms: Vision, hype, and reality for delivering computing as the 5th utility,” <em>Future Generation computer systems</em>, vol. 25, pp. 599–616, 2009.</p>
<p>[3] OpenStack LLC, “OpenStack: The Open Source Cloud Operating System,” 21-Jul-2012. [Online]. Available: <a href="http://www.openstack.org/software/" title="http://www.openstack.org/software/">http://www.openstack.org/software/</a>.</p>
<p>[4] OpenStack LLC, “OpenStack Compute Administration Manual,” 2012.</p>
<p>[5] OpenStack LLC, “OpenStack Install and Deploy Manual,” 2012.</p>
<p>[6] R. Landmann, J. Reed, D. Cantrell, H. D. Goede, and J. Masters, “Red Hat Enterprise Linux 6 Installation Guide,” 2012.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Amazon EC2. <a href="http://aws.amazon.com/ec2/">http://aws.amazon.com/ec2/</a>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Google Compute Engine. <a href="http://cloud.google.com/products/compute-engine.html">http://cloud.google.com/products/compute-engine.html</a>.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Google App Engine. <a href="http://cloud.google.com/products/">http://cloud.google.com/products/</a>.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Microsoft Azure. <a href="http://www.windowsazure.com/">http://www.windowsazure.com/</a>.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>Salesforce.com. <a href="http://www.salesforce.com/">http://www.salesforce.com/</a>.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>Amazon Web Services Marketplace. <a href="https://aws.amazon.com/marketplace/">https://aws.amazon.com/marketplace/</a>.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>The project repository. <a href="https://github.com/beloglazov/openstack-centos-kvm-glusterfs">https://github.com/beloglazov/openstack-centos-kvm-glusterfs</a>.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>CentOS. <a href="http://centos.org/">http://centos.org/</a>.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>GlusterFS. <a href="http://gluster.org/">http://gluster.org/</a>.<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>KVM. <a href="http://www.linux-kvm.org/">http://www.linux-kvm.org/</a>.<a href="#fnref10">↩</a></p></li>
<li id="fn11"><p>OpenStack. <a href="http://openstack.org/">http://openstack.org/</a>.<a href="#fnref11">↩</a></p></li>
<li id="fn12"><p>The OpenStack Foundation. <a href="http://wiki.openstack.org/Governance/Foundation/Structure">http://wiki.openstack.org/Governance/Foundation/Structure</a>.<a href="#fnref12">↩</a></p></li>
<li id="fn13"><p>Libvirt. <a href="http://libvirt.org/">http://libvirt.org/</a>.<a href="#fnref13">↩</a></p></li>
<li id="fn14"><p><a href="http://devstack.org/">Http://devstack.org/</a>.<a href="#fnref14">↩</a></p></li>
<li id="fn15"><p>Http://docs.openstack.org/trunk/openstack-compute/admin/content/openstack-compute-deployment-tool-with-puppet.html.<a href="#fnref15">↩</a></p></li>
<li id="fn16"><p>The project repository. <a href="https://github.com/beloglazov/openstack-centos-kvm-glusterfs">https://github.com/beloglazov/openstack-centos-kvm-glusterfs</a>.<a href="#fnref16">↩</a></p></li>
<li id="fn17"><p>XFS. <a href="http://en.wikipedia.org/wiki/XFS">http://en.wikipedia.org/wiki/XFS</a>.<a href="#fnref17">↩</a></p></li>
<li id="fn18"><p>Git. <a href="http://git-scm.com/">http://git-scm.com/</a>.<a href="#fnref18">↩</a></p></li>
<li id="fn19"><p>SELinux. <a href="http://en.wikipedia.org/wiki/Security-Enhanced_Linux">http://en.wikipedia.org/wiki/Security-Enhanced_Linux</a>.<a href="#fnref19">↩</a></p></li>
<li id="fn20"><p>Libvirt. <a href="http://libvirt.org/">http://libvirt.org/</a>.<a href="#fnref20">↩</a></p></li>
<li id="fn21"><p>The EPEL repository. <a href="http://fedoraproject.org/wiki/EPEL">http://fedoraproject.org/wiki/EPEL</a>.<a href="#fnref21">↩</a></p></li>
<li id="fn22"><p>The <em>keystone-init</em> project. <a href="https://github.com/nimbis/keystone-init">https://github.com/nimbis/keystone-init</a>.<a href="#fnref22">↩</a></p></li>
<li id="fn23"><p>YAML. <a href="http://en.wikipedia.org/wiki/YAML">http://en.wikipedia.org/wiki/YAML</a>.<a href="#fnref23">↩</a></p></li>
<li id="fn24"><p>CirrOS. <a href="https://launchpad.net/cirros/">https://launchpad.net/cirros/</a>.<a href="#fnref24">↩</a></p></li>
<li id="fn25"><p>Ubuntu Cloud Images. <a href="http://uec-images.ubuntu.com/">http://uec-images.ubuntu.com/</a>.<a href="#fnref25">↩</a></p></li>
<li id="fn26"><p>QEMU. <a href="http://en.wikipedia.org/wiki/QEMU">http://en.wikipedia.org/wiki/QEMU</a>.<a href="#fnref26">↩</a></p></li>
<li id="fn27"><p>Dnsmasq. <a href="http://en.wikipedia.org/wiki/Dnsmasq">http://en.wikipedia.org/wiki/Dnsmasq</a>.<a href="#fnref27">↩</a></p></li>
<li id="fn28"><p>OpenStack Compute Questions. <a href="https://answers.launchpad.net/nova/+question/200985">https://answers.launchpad.net/nova/+question/200985</a>.<a href="#fnref28">↩</a></p></li>
</ol>
</div>
